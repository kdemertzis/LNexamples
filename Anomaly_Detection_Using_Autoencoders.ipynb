{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anomaly Detection Using Autoencoders.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdemertzis/LNexamples/blob/main/Anomaly_Detection_Using_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Np6RCC8nbAJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL7Kir1Jvwzc"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "ATDPsAdxnsr6",
        "outputId": "f0b39282-568c-4bdc-d42d-6a55bb55ff29"
      },
      "source": [
        "# Download the dataset\n",
        "PATH_TO_DATA = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'\n",
        "data = pd.read_csv(PATH_TO_DATA, header=None)\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4998, 141)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112522</td>\n",
              "      <td>-2.827204</td>\n",
              "      <td>-3.773897</td>\n",
              "      <td>-4.349751</td>\n",
              "      <td>-4.376041</td>\n",
              "      <td>-3.474986</td>\n",
              "      <td>-2.181408</td>\n",
              "      <td>-1.818287</td>\n",
              "      <td>-1.250522</td>\n",
              "      <td>-0.477492</td>\n",
              "      <td>-0.363808</td>\n",
              "      <td>-0.491957</td>\n",
              "      <td>-0.421855</td>\n",
              "      <td>-0.309201</td>\n",
              "      <td>-0.495939</td>\n",
              "      <td>-0.342119</td>\n",
              "      <td>-0.355336</td>\n",
              "      <td>-0.367913</td>\n",
              "      <td>-0.316503</td>\n",
              "      <td>-0.412374</td>\n",
              "      <td>-0.471672</td>\n",
              "      <td>-0.413458</td>\n",
              "      <td>-0.364617</td>\n",
              "      <td>-0.449298</td>\n",
              "      <td>-0.471419</td>\n",
              "      <td>-0.424777</td>\n",
              "      <td>-0.462517</td>\n",
              "      <td>-0.552472</td>\n",
              "      <td>-0.475375</td>\n",
              "      <td>-0.694200</td>\n",
              "      <td>-0.701868</td>\n",
              "      <td>-0.593812</td>\n",
              "      <td>-0.660684</td>\n",
              "      <td>-0.713831</td>\n",
              "      <td>-0.769807</td>\n",
              "      <td>-0.672282</td>\n",
              "      <td>-0.653676</td>\n",
              "      <td>-0.639406</td>\n",
              "      <td>-0.559302</td>\n",
              "      <td>-0.591670</td>\n",
              "      <td>...</td>\n",
              "      <td>1.258179</td>\n",
              "      <td>1.433789</td>\n",
              "      <td>1.700533</td>\n",
              "      <td>1.999043</td>\n",
              "      <td>2.125341</td>\n",
              "      <td>1.993291</td>\n",
              "      <td>1.932246</td>\n",
              "      <td>1.797437</td>\n",
              "      <td>1.522284</td>\n",
              "      <td>1.251168</td>\n",
              "      <td>0.998730</td>\n",
              "      <td>0.483722</td>\n",
              "      <td>0.023132</td>\n",
              "      <td>-0.194914</td>\n",
              "      <td>-0.220917</td>\n",
              "      <td>-0.243737</td>\n",
              "      <td>-0.254695</td>\n",
              "      <td>-0.291136</td>\n",
              "      <td>-0.256490</td>\n",
              "      <td>-0.227874</td>\n",
              "      <td>-0.322423</td>\n",
              "      <td>-0.289286</td>\n",
              "      <td>-0.318170</td>\n",
              "      <td>-0.363654</td>\n",
              "      <td>-0.393456</td>\n",
              "      <td>-0.266419</td>\n",
              "      <td>-0.256823</td>\n",
              "      <td>-0.288694</td>\n",
              "      <td>-0.162338</td>\n",
              "      <td>0.160348</td>\n",
              "      <td>0.792168</td>\n",
              "      <td>0.933541</td>\n",
              "      <td>0.796958</td>\n",
              "      <td>0.578621</td>\n",
              "      <td>0.257740</td>\n",
              "      <td>0.228077</td>\n",
              "      <td>0.123431</td>\n",
              "      <td>0.925286</td>\n",
              "      <td>0.193137</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.100878</td>\n",
              "      <td>-3.996840</td>\n",
              "      <td>-4.285843</td>\n",
              "      <td>-4.506579</td>\n",
              "      <td>-4.022377</td>\n",
              "      <td>-3.234368</td>\n",
              "      <td>-1.566126</td>\n",
              "      <td>-0.992258</td>\n",
              "      <td>-0.754680</td>\n",
              "      <td>0.042321</td>\n",
              "      <td>0.148951</td>\n",
              "      <td>0.183527</td>\n",
              "      <td>0.294876</td>\n",
              "      <td>0.190233</td>\n",
              "      <td>0.235575</td>\n",
              "      <td>0.253487</td>\n",
              "      <td>0.221742</td>\n",
              "      <td>0.050233</td>\n",
              "      <td>0.178042</td>\n",
              "      <td>0.139563</td>\n",
              "      <td>0.046794</td>\n",
              "      <td>0.043007</td>\n",
              "      <td>0.106544</td>\n",
              "      <td>0.012654</td>\n",
              "      <td>0.003995</td>\n",
              "      <td>0.045724</td>\n",
              "      <td>-0.045999</td>\n",
              "      <td>-0.072667</td>\n",
              "      <td>-0.071078</td>\n",
              "      <td>-0.153866</td>\n",
              "      <td>-0.227254</td>\n",
              "      <td>-0.249270</td>\n",
              "      <td>-0.253489</td>\n",
              "      <td>-0.332835</td>\n",
              "      <td>-0.264330</td>\n",
              "      <td>-0.345825</td>\n",
              "      <td>-0.310781</td>\n",
              "      <td>-0.334160</td>\n",
              "      <td>-0.306178</td>\n",
              "      <td>-0.174563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.808428</td>\n",
              "      <td>2.164346</td>\n",
              "      <td>2.070747</td>\n",
              "      <td>1.903614</td>\n",
              "      <td>1.764455</td>\n",
              "      <td>1.507769</td>\n",
              "      <td>1.293428</td>\n",
              "      <td>0.894562</td>\n",
              "      <td>0.578016</td>\n",
              "      <td>0.244343</td>\n",
              "      <td>-0.286443</td>\n",
              "      <td>-0.515881</td>\n",
              "      <td>-0.732707</td>\n",
              "      <td>-0.832465</td>\n",
              "      <td>-0.803318</td>\n",
              "      <td>-0.836252</td>\n",
              "      <td>-0.777865</td>\n",
              "      <td>-0.774753</td>\n",
              "      <td>-0.733404</td>\n",
              "      <td>-0.721386</td>\n",
              "      <td>-0.832095</td>\n",
              "      <td>-0.711982</td>\n",
              "      <td>-0.751867</td>\n",
              "      <td>-0.757720</td>\n",
              "      <td>-0.853120</td>\n",
              "      <td>-0.766988</td>\n",
              "      <td>-0.688161</td>\n",
              "      <td>-0.519923</td>\n",
              "      <td>0.039406</td>\n",
              "      <td>0.560327</td>\n",
              "      <td>0.538356</td>\n",
              "      <td>0.656881</td>\n",
              "      <td>0.787490</td>\n",
              "      <td>0.724046</td>\n",
              "      <td>0.555784</td>\n",
              "      <td>0.476333</td>\n",
              "      <td>0.773820</td>\n",
              "      <td>1.119621</td>\n",
              "      <td>-1.436250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.567088</td>\n",
              "      <td>-2.593450</td>\n",
              "      <td>-3.874230</td>\n",
              "      <td>-4.584095</td>\n",
              "      <td>-4.187449</td>\n",
              "      <td>-3.151462</td>\n",
              "      <td>-1.742940</td>\n",
              "      <td>-1.490658</td>\n",
              "      <td>-1.183580</td>\n",
              "      <td>-0.394229</td>\n",
              "      <td>-0.282897</td>\n",
              "      <td>-0.356926</td>\n",
              "      <td>-0.287297</td>\n",
              "      <td>-0.399489</td>\n",
              "      <td>-0.473244</td>\n",
              "      <td>-0.379048</td>\n",
              "      <td>-0.399039</td>\n",
              "      <td>-0.178594</td>\n",
              "      <td>-0.339522</td>\n",
              "      <td>-0.498447</td>\n",
              "      <td>-0.337251</td>\n",
              "      <td>-0.425480</td>\n",
              "      <td>-0.423952</td>\n",
              "      <td>-0.463170</td>\n",
              "      <td>-0.493253</td>\n",
              "      <td>-0.549749</td>\n",
              "      <td>-0.529831</td>\n",
              "      <td>-0.530935</td>\n",
              "      <td>-0.502365</td>\n",
              "      <td>-0.417368</td>\n",
              "      <td>-0.526346</td>\n",
              "      <td>-0.471005</td>\n",
              "      <td>-0.676784</td>\n",
              "      <td>-0.898612</td>\n",
              "      <td>-0.610571</td>\n",
              "      <td>-0.530164</td>\n",
              "      <td>-0.765674</td>\n",
              "      <td>-0.581937</td>\n",
              "      <td>-0.537848</td>\n",
              "      <td>-0.556386</td>\n",
              "      <td>...</td>\n",
              "      <td>1.810988</td>\n",
              "      <td>2.185398</td>\n",
              "      <td>2.262985</td>\n",
              "      <td>2.052920</td>\n",
              "      <td>1.890488</td>\n",
              "      <td>1.793033</td>\n",
              "      <td>1.564784</td>\n",
              "      <td>1.234619</td>\n",
              "      <td>0.900302</td>\n",
              "      <td>0.551957</td>\n",
              "      <td>0.258222</td>\n",
              "      <td>-0.128587</td>\n",
              "      <td>-0.092585</td>\n",
              "      <td>-0.168606</td>\n",
              "      <td>-0.495989</td>\n",
              "      <td>-0.395034</td>\n",
              "      <td>-0.328238</td>\n",
              "      <td>-0.448138</td>\n",
              "      <td>-0.268230</td>\n",
              "      <td>-0.456415</td>\n",
              "      <td>-0.357867</td>\n",
              "      <td>-0.317508</td>\n",
              "      <td>-0.434112</td>\n",
              "      <td>-0.549203</td>\n",
              "      <td>-0.324615</td>\n",
              "      <td>-0.268082</td>\n",
              "      <td>-0.220384</td>\n",
              "      <td>-0.117429</td>\n",
              "      <td>0.614059</td>\n",
              "      <td>1.284825</td>\n",
              "      <td>0.886073</td>\n",
              "      <td>0.531452</td>\n",
              "      <td>0.311377</td>\n",
              "      <td>-0.021919</td>\n",
              "      <td>-0.713683</td>\n",
              "      <td>-0.532197</td>\n",
              "      <td>0.321097</td>\n",
              "      <td>0.904227</td>\n",
              "      <td>-0.421797</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.490473</td>\n",
              "      <td>-1.914407</td>\n",
              "      <td>-3.616364</td>\n",
              "      <td>-4.318823</td>\n",
              "      <td>-4.268016</td>\n",
              "      <td>-3.881110</td>\n",
              "      <td>-2.993280</td>\n",
              "      <td>-1.671131</td>\n",
              "      <td>-1.333884</td>\n",
              "      <td>-0.965629</td>\n",
              "      <td>-0.183319</td>\n",
              "      <td>-0.101657</td>\n",
              "      <td>-0.273874</td>\n",
              "      <td>-0.127818</td>\n",
              "      <td>-0.195983</td>\n",
              "      <td>-0.213523</td>\n",
              "      <td>-0.176473</td>\n",
              "      <td>-0.156932</td>\n",
              "      <td>-0.149172</td>\n",
              "      <td>-0.181510</td>\n",
              "      <td>-0.180074</td>\n",
              "      <td>-0.246151</td>\n",
              "      <td>-0.274260</td>\n",
              "      <td>-0.140960</td>\n",
              "      <td>-0.277449</td>\n",
              "      <td>-0.382549</td>\n",
              "      <td>-0.311937</td>\n",
              "      <td>-0.360093</td>\n",
              "      <td>-0.405968</td>\n",
              "      <td>-0.571433</td>\n",
              "      <td>-0.524106</td>\n",
              "      <td>-0.537886</td>\n",
              "      <td>-0.606778</td>\n",
              "      <td>-0.661446</td>\n",
              "      <td>-0.683375</td>\n",
              "      <td>-0.746683</td>\n",
              "      <td>-0.635662</td>\n",
              "      <td>-0.625231</td>\n",
              "      <td>-0.540094</td>\n",
              "      <td>-0.674995</td>\n",
              "      <td>...</td>\n",
              "      <td>1.772155</td>\n",
              "      <td>2.000769</td>\n",
              "      <td>1.925003</td>\n",
              "      <td>1.898426</td>\n",
              "      <td>1.720953</td>\n",
              "      <td>1.501711</td>\n",
              "      <td>1.422492</td>\n",
              "      <td>1.023225</td>\n",
              "      <td>0.776341</td>\n",
              "      <td>0.504426</td>\n",
              "      <td>0.056382</td>\n",
              "      <td>-0.233161</td>\n",
              "      <td>-0.406388</td>\n",
              "      <td>-0.327528</td>\n",
              "      <td>-0.460868</td>\n",
              "      <td>-0.402536</td>\n",
              "      <td>-0.345752</td>\n",
              "      <td>-0.354206</td>\n",
              "      <td>-0.439959</td>\n",
              "      <td>-0.425326</td>\n",
              "      <td>-0.439789</td>\n",
              "      <td>-0.451835</td>\n",
              "      <td>-0.395926</td>\n",
              "      <td>-0.448762</td>\n",
              "      <td>-0.391789</td>\n",
              "      <td>-0.376307</td>\n",
              "      <td>-0.461069</td>\n",
              "      <td>-0.253524</td>\n",
              "      <td>0.213006</td>\n",
              "      <td>0.491173</td>\n",
              "      <td>0.350816</td>\n",
              "      <td>0.499111</td>\n",
              "      <td>0.600345</td>\n",
              "      <td>0.842069</td>\n",
              "      <td>0.952074</td>\n",
              "      <td>0.990133</td>\n",
              "      <td>1.086798</td>\n",
              "      <td>1.403011</td>\n",
              "      <td>-0.383564</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.800232</td>\n",
              "      <td>-0.874252</td>\n",
              "      <td>-2.384761</td>\n",
              "      <td>-3.973292</td>\n",
              "      <td>-4.338224</td>\n",
              "      <td>-3.802422</td>\n",
              "      <td>-2.534510</td>\n",
              "      <td>-1.783423</td>\n",
              "      <td>-1.594450</td>\n",
              "      <td>-0.753199</td>\n",
              "      <td>-0.298107</td>\n",
              "      <td>-0.428928</td>\n",
              "      <td>-0.491351</td>\n",
              "      <td>-0.361304</td>\n",
              "      <td>-0.339296</td>\n",
              "      <td>-0.324952</td>\n",
              "      <td>-0.290113</td>\n",
              "      <td>-0.363051</td>\n",
              "      <td>-0.525684</td>\n",
              "      <td>-0.597423</td>\n",
              "      <td>-0.575523</td>\n",
              "      <td>-0.567503</td>\n",
              "      <td>-0.504555</td>\n",
              "      <td>-0.618406</td>\n",
              "      <td>-0.682814</td>\n",
              "      <td>-0.743849</td>\n",
              "      <td>-0.815588</td>\n",
              "      <td>-0.826902</td>\n",
              "      <td>-0.782374</td>\n",
              "      <td>-0.929462</td>\n",
              "      <td>-0.999672</td>\n",
              "      <td>-1.060969</td>\n",
              "      <td>-1.007877</td>\n",
              "      <td>-1.028735</td>\n",
              "      <td>-1.122629</td>\n",
              "      <td>-1.028650</td>\n",
              "      <td>-1.046515</td>\n",
              "      <td>-1.063372</td>\n",
              "      <td>-1.122423</td>\n",
              "      <td>-0.983242</td>\n",
              "      <td>...</td>\n",
              "      <td>1.155363</td>\n",
              "      <td>1.336254</td>\n",
              "      <td>1.627534</td>\n",
              "      <td>1.717594</td>\n",
              "      <td>1.696487</td>\n",
              "      <td>1.741686</td>\n",
              "      <td>1.674078</td>\n",
              "      <td>1.546928</td>\n",
              "      <td>1.331738</td>\n",
              "      <td>1.110168</td>\n",
              "      <td>0.922210</td>\n",
              "      <td>0.521777</td>\n",
              "      <td>0.154852</td>\n",
              "      <td>-0.123861</td>\n",
              "      <td>-0.202998</td>\n",
              "      <td>-0.247956</td>\n",
              "      <td>-0.219122</td>\n",
              "      <td>-0.214695</td>\n",
              "      <td>-0.319215</td>\n",
              "      <td>-0.198597</td>\n",
              "      <td>-0.151618</td>\n",
              "      <td>-0.129593</td>\n",
              "      <td>-0.074939</td>\n",
              "      <td>-0.196807</td>\n",
              "      <td>-0.174795</td>\n",
              "      <td>-0.208833</td>\n",
              "      <td>-0.210754</td>\n",
              "      <td>-0.100485</td>\n",
              "      <td>0.197446</td>\n",
              "      <td>0.966606</td>\n",
              "      <td>1.148884</td>\n",
              "      <td>0.958434</td>\n",
              "      <td>1.059025</td>\n",
              "      <td>1.371682</td>\n",
              "      <td>1.277392</td>\n",
              "      <td>0.960304</td>\n",
              "      <td>0.971020</td>\n",
              "      <td>1.614392</td>\n",
              "      <td>1.421456</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3    ...       137       138       139  140\n",
              "0 -0.112522 -2.827204 -3.773897 -4.349751  ...  0.123431  0.925286  0.193137  1.0\n",
              "1 -1.100878 -3.996840 -4.285843 -4.506579  ...  0.773820  1.119621 -1.436250  1.0\n",
              "2 -0.567088 -2.593450 -3.874230 -4.584095  ...  0.321097  0.904227 -0.421797  1.0\n",
              "3  0.490473 -1.914407 -3.616364 -4.318823  ...  1.086798  1.403011 -0.383564  1.0\n",
              "4  0.800232 -0.874252 -2.384761 -3.973292  ...  0.971020  1.614392  1.421456  1.0\n",
              "\n",
              "[5 rows x 141 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6KUkRgvoOK"
      },
      "source": [
        "## Split the data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTEDLRtA4I9k"
      },
      "source": [
        "# last column is the target\n",
        "# 0 = anomaly, 1 = normal\n",
        "TARGET = 140\n",
        "\n",
        "features = data.drop(TARGET, axis=1)\n",
        "target = data[TARGET]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    features, target, test_size=0.2, stratify=target\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mjzHNYjLviR"
      },
      "source": [
        "# use case is novelty detection so use only the normal data\n",
        "# for training\n",
        "train_index = y_train[y_train == 1].index\n",
        "train_data = x_train.loc[train_index]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfk5mwbhv03v"
      },
      "source": [
        "## Scale the data using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmJhTiuBts4F"
      },
      "source": [
        "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
        "x_test_scaled = min_max_scaler.transform(x_test.copy())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFLk9eJrwqvl"
      },
      "source": [
        "## Build an AutoEncoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9VmLu89uPIY"
      },
      "source": [
        "# create a model by subclassing Model class in tensorflow\n",
        "class AutoEncoder(Model):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  output_units: int\n",
        "    Number of output units\n",
        "  \n",
        "  code_size: int\n",
        "    Number of units in bottle neck\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_units, code_size=8):\n",
        "    super().__init__()\n",
        "    self.encoder = Sequential([\n",
        "      Dense(64, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(16, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(64, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oylLrsQuPD5",
        "outputId": "52b2db9a-62a2-4eac-ba14-a8054c4e4307"
      },
      "source": [
        "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
        "# configurations of model\n",
        "model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0106 - mse: 0.0237 - val_loss: 0.0131 - val_mse: 0.0304\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0103 - mse: 0.0230 - val_loss: 0.0129 - val_mse: 0.0300\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0098 - mse: 0.0220 - val_loss: 0.0127 - val_mse: 0.0295\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0089 - mse: 0.0198 - val_loss: 0.0126 - val_mse: 0.0292\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0079 - mse: 0.0176 - val_loss: 0.0121 - val_mse: 0.0282\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0071 - mse: 0.0157 - val_loss: 0.0114 - val_mse: 0.0265\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0064 - mse: 0.0142 - val_loss: 0.0113 - val_mse: 0.0262\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0059 - mse: 0.0130 - val_loss: 0.0106 - val_mse: 0.0246\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0054 - mse: 0.0121 - val_loss: 0.0105 - val_mse: 0.0244\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0052 - mse: 0.0115 - val_loss: 0.0102 - val_mse: 0.0238\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0050 - mse: 0.0110 - val_loss: 0.0102 - val_mse: 0.0239\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - mse: 0.0108 - val_loss: 0.0101 - val_mse: 0.0236\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - mse: 0.0106 - val_loss: 0.0100 - val_mse: 0.0234\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0047 - mse: 0.0104 - val_loss: 0.0100 - val_mse: 0.0234\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0046 - mse: 0.0103 - val_loss: 0.0100 - val_mse: 0.0235\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0046 - mse: 0.0102 - val_loss: 0.0100 - val_mse: 0.0235\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0045 - mse: 0.0101 - val_loss: 0.0100 - val_mse: 0.0235\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0045 - mse: 0.0100 - val_loss: 0.0100 - val_mse: 0.0236\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0100 - val_mse: 0.0236\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0100 - val_mse: 0.0235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCD6ykuG6wvV"
      },
      "source": [
        "## Plot history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ohEpArnj6NMG",
        "outputId": "86570da8-8cd3-4c69-f247-671a2273d57c"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSLE Loss')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPNtOnsENWwc"
      },
      "source": [
        "## Find threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACFZcFWEFNpQ"
      },
      "source": [
        "def find_threshold(model, x_train_scaled):\n",
        "  reconstructions = model.predict(x_train_scaled)\n",
        "  # provides losses of individual instances\n",
        "  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
        "\n",
        "  # threshold for anomaly scores\n",
        "  threshold = np.mean(reconstruction_errors.numpy()) \\\n",
        "      + np.std(reconstruction_errors.numpy())\n",
        "  return threshold\n",
        "\n",
        "def find_threshold_method_two(model, x_train_scaled):\n",
        "  # another method to find threshold\n",
        "  reconstructions = model.predict(x_train_scaled)\n",
        "  # provides losses of individual instances\n",
        "  reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
        "\n",
        "  threshold_2 = np.percentile(reconstruction_errors, 95)\n",
        "  return threshold_2\n",
        "\n",
        "def get_predictions(model, x_test_scaled, threshold):\n",
        "  predictions = model.predict(x_test_scaled)\n",
        "  # provides losses of individual instances\n",
        "  errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
        "  # 0 = anomaly, 1 = normal\n",
        "  anomaly_mask = pd.Series(errors) > threshold\n",
        "  preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
        "  return preds"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Mxr7cnHPk-",
        "outputId": "bd915948-02ac-4c8a-89a5-7dd4578ecce1"
      },
      "source": [
        "threshold = find_threshold(model, x_train_scaled)\n",
        "print(f\"Threshold method one: {threshold}\")\n",
        "\n",
        "threshold_2 = find_threshold_method_two(model, x_train_scaled)\n",
        "print(f\"Threshold method two: {threshold_2}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold method one: 0.009611773533172222\n",
            "Threshold method two: 0.013043497814127022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl0Op0GNHVDD",
        "outputId": "17ee7fa7-eb0d-478a-bc29-96b742428487"
      },
      "source": [
        "preds = get_predictions(model, x_test_scaled, threshold)\n",
        "accuracy_score(preds, y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.928"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSdu3Uk7ASm"
      },
      "source": [
        "## Tuning AutoEncoder using keras tuner\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zt5CaPNCh__",
        "outputId": "5c04e6f7-b31a-4416-fb87-5a71f398857a"
      },
      "source": [
        "!pip install -U keras-tuner"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–                            | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 20 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 30 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 40 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 51 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 61 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 71 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 81 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 92 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.0.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.4 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AwQid85DbyC",
        "outputId": "bb72b357-35ef-4988-82e5-091ebb039359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import kerastuner as kt\n",
        "\n",
        "class AutoEncoderTuner(Model):\n",
        "\n",
        "  def __init__(self, hp, output_units, code_size=8):\n",
        "    super().__init__()\n",
        "    dense_1_units = hp.Int('dense_1_units', min_value=16, max_value=72, step=4)\n",
        "    dense_2_units = hp.Int('dense_2_units', min_value=16, max_value=72, step=4)\n",
        "    dense_3_units = hp.Int('dense_3_units', min_value=16, max_value=72, step=4)\n",
        "    dense_4_units = hp.Int('dense_4_units', min_value=16, max_value=72, step=4)\n",
        "    dense_5_units = hp.Int('dense_5_units', min_value=16, max_value=72, step=4)\n",
        "    dense_6_units = hp.Int('dense_6_units', min_value=16, max_value=72, step=4)\n",
        "    \n",
        "    self.encoder = Sequential([\n",
        "      Dense(dense_1_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_2_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_3_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(code_size, activation='relu')\n",
        "    ])\n",
        "    self.decoder = Sequential([\n",
        "      Dense(dense_4_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_5_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(dense_6_units, activation='relu'),\n",
        "      Dropout(0.1),\n",
        "      Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    encoded = self.encoder(inputs)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  model = AutoEncoderTuner(hp, 140)\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  model.compile(\n",
        "      loss='msle',\n",
        "      optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6X0KEcX6NAS",
        "outputId": "79d584b5-3c00-4b3f-f5d5-d93a9943b123"
      },
      "source": [
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='autoencoder',\n",
        "    project_name='tuning_autoencoder6'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train_scaled, \n",
        "    x_train_scaled, \n",
        "    epochs=20, \n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 02s]\n",
            "val_loss: 0.009157148189842701\n",
            "\n",
            "Best val_loss So Far: 0.008683794178068638\n",
            "Total elapsed time: 00h 00m 57s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tiB8zhcF0QV",
        "outputId": "5d419195-999f-4e2a-b556-353628389887"
      },
      "source": [
        "hparams = [f'dense_{i}_units' for i in range(1,7)] + ['learning_rate']\n",
        "best_hyperparams = tuner.get_best_hyperparameters()\n",
        "for hps in hparams:\n",
        "  print(f\"{hps}: {best_hyperparams[0][hps]}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense_1_units: 44\n",
            "dense_2_units: 48\n",
            "dense_3_units: 20\n",
            "dense_4_units: 68\n",
            "dense_5_units: 60\n",
            "dense_6_units: 48\n",
            "learning_rate: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUvDIqvVEuNk",
        "outputId": "8511485e-11aa-4ef1-ec77-03ab7e279ef8"
      },
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "best_model.compile(loss='msle', optimizer=Adam(0.001))\n",
        "\n",
        "best_model.fit(\n",
        "    x_train_scaled,\n",
        "    x_train_scaled,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test_scaled, x_test_scaled)\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 1s 49ms/step - loss: 0.0029 - val_loss: 0.0087\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0088\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0088\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0087\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0088\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0088\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0028 - val_loss: 0.0088\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0088\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0088\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0087\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0088\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0088\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0087\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0027 - val_loss: 0.0087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2eee1e8390>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0DvGEOkHdsn",
        "outputId": "f28d278b-ee15-4b84-f762-5918a85e392f"
      },
      "source": [
        "threshold_ = find_threshold(best_model, x_train_scaled)\n",
        "preds_ = get_predictions(best_model, x_test_scaled, threshold_)\n",
        "accuracy_score(preds_, y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.926"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}