{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Τρία παραδείγματα ταξινομητών.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdemertzis/LNexamples/blob/main/%CE%A4%CF%81%CE%AF%CE%B1_%CF%80%CE%B1%CF%81%CE%B1%CE%B4%CE%B5%CE%AF%CE%B3%CE%BC%CE%B1%CF%84%CE%B1_%CF%84%CE%B1%CE%BE%CE%B9%CE%BD%CE%BF%CE%BC%CE%B7%CF%84%CF%8E%CE%BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoIlBu68NC54"
      },
      "source": [
        "Αρχικά κάνουμε upgrade στις βιβλοθήκες που θα χρειαστούμε, αν δεν το έχουμε ήδη κάνει. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wVeTFMhlNC55",
        "trusted": true
      },
      "source": [
        "!pip install --upgrade pip #upgrade pip package installer\n",
        "!pip install scikit-learn --upgrade #upgrade scikit-learn package\n",
        "!pip install numpy --upgrade #upgrade numpy package\n",
        "!pip install --upgrade matplotlib # Κάνουμε update την matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtGvRuQMNC5_"
      },
      "source": [
        "## Εισαγωγή dataset και διαχωρισμός σε train και test set\n",
        "\n",
        "Εισάγουμε το [Breast Cancer Wisconsin Diagnostic Database](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). Το dataset περιλαμβάνει διάφορες πληροφορίες για όγκους σχετιζόμενους με τον καρκίνο του στήθους καθώς και ετικέτες για κάθε δείγμα (sample), αν το δείγμα αντιστοιχεί σε καλοήθη όγκο ή κακοήθη. Το σύνολο δεδομένων έχει 569 δείγματα για αντίστοιχους όγκους και περιλαμβάνει 30 χαρακτηριστικά (attributes) για κάθε δείγμα, όπως ακτίνα του όγκου, υφή, ομοιομορφία και περιοχή. Θα χρησιμοποιήσουμε αυτό το dataset και τα χαρακτηριστικά για να προβλέψουμε αν ένας όγκος είναι κακοήθης ή όχι."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZocYkGPENC6A",
        "trusted": true
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "label_names = data['target_names']\n",
        "labels = data['target']\n",
        "feature_names = data['feature_names']\n",
        "features = data['data']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split our data\n",
        "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Q3OlQo71OQaF"
      },
      "source": [
        "# Παραγωγικοί ταξινομητές\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB62Ys3PiH3B"
      },
      "source": [
        "\n",
        "## Naive Bayes Classifier (κατηγορικές μεταβλητές)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ9YJR8aOQaK"
      },
      "source": [
        "![$P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}$](https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Bayes%27_Theorem_MMB_01.jpg/220px-Bayes%27_Theorem_MMB_01.jpg \"A blue neon sign, showing the simple statement of Bayes’ theorem\")\n",
        "\n",
        "H βασική ιδέα λειτουργίας του ταξινομητή είναι α) ο γνωστός νόμος του Bayes $$P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}$$\n",
        "\n",
        "και β) η (naive) υπόθεση ότι τα χαρακτηριστικά είναι όλα ανεξάρτητα μεταξύ τους (δεν ισχύει γενικά, αλλά ο ταξινομητής είναι πρακτικά καλός σε πολλές περιπτώσεις). Παράδειγμα: θα βρέξει σήμερα? Naive Bayes: \"Θα το προβλέψω με βάση το παρελθόν θεωρώντας ότι τα χαρακτηριστικά θερμοκρασία, νεφοκάλυψη και ατμοσφαιρική πίεση είναι όλα ανεξάρτητα μεταξύ τους\".\n",
        "\n",
        "Με δεδομένα μια μεταβλητή κατηγορίας (κλάσης) $y$ και ένα εξαρτώμενο διάνυσμα χαρακτηριστικών $x_1$ μέχρι $x_n$, σύμφωνα με το θεώρημα του Bayes θα ισχύει \n",
        "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}{P(x_1, \\dots, x_n)}$$\n",
        "Ισχύει ότι $P(x_1, \\dots, x_i, \\dots, x_n \\mid y) =  \\prod_{i=1}^{n} P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n)$ και κάνουμε την αφελή υπόθεση ότι το χαρακτηριστικό $x_i$ για κάθε $i$ εξαρτάται μόνο από την κλάση $y$ και όχι από οποιοδήποτε άλλο χαρακτηριστικό\n",
        "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$$\n",
        "αυτό οδηγεί στην απλοποίηση\n",
        "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}$$\n",
        "Με δεδομένη είσοδο, το $P(x_1, \\dots, x_n)$ είναι σταθερό. Συνεπώς μπορούμε να χρησιμοποιήσουμε τον ακόλουθο κανόνα ταξινόμησης $$P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
        "$$\\Downarrow$$\n",
        "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
        "Το $P(y)$ είναι η υπόθεσή μας και ισούται με τη σχετική συχνότητα της κλάσης $y$ στο training set. To $P(x_i \\mid y)$ είναι η πιθανοφάνεια δηλαδή η πιθανότητα του δείγματος με δεδομένη την υπόθεσή μας και μπορεί επίσης να υπολογιστεί απλά από το training set. Οι διάφοροι Naive Bayes classifiers διαφοροποιούνται κυρίως από τις υποθέσεις που κάνουν ως προς την κατανομή $P(x_i \\mid y)$. Η κλάση $\\hat{y}$ που ανατίθεται σε ένα νέο δείγμα είναι αυτή που μεγιστοποιεί το δεξί μέλος της σχέσης."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8k9HnDOQaL"
      },
      "source": [
        "### Ένα παράδειγμα\n",
        "\n",
        "Έστω ότι για 14 μέρες παρατηρήσαμε 4 μεταβλητές του καιρού (νεφοκάλυψη, θερμοκρασία, υγρασία και άνεμο) και το αν τελικά παίξαμε τέννις. Τα χαρακτηριστικά μας είναι κατηγορικά, παίρνουν δηλαδή διακριτές τιμές από ενα ορισμένο σύνολο τιμών.\n",
        "\n",
        "| Day | Outlook  | Temperature | Humidity | Wind   | Play Tennis? |\n",
        "|-----|----------|-------------|----------|--------|--------------|\n",
        "| 1   | Sunny    | Hot         | High     | Weak   | No           |\n",
        "| 2   | Sunny    | Hot         | High     | Strong | No           |\n",
        "| 3   | Overcast | Hot         | High     | Weak   | Yes          |\n",
        "| 4   | Rain     | Mild        | High     | Weak   | Yes          |\n",
        "| 5   | Rain     | Cool        | Normal   | Weak   | Yes          |\n",
        "| 6   | Rain     | Cool        | Normal   | Strong | No           |\n",
        "| 7   | Overcast | Cool        | Normal   | Strong | Yes          |\n",
        "| 8   | Sunny    | Mild        | High     | Weak   | No           |\n",
        "| 9   | Sunny    | Cool        | Normal   | Weak   | Yes          |\n",
        "| 10  | Rain     | Mild        | Normal   | Weak   | Yes          |\n",
        "| 11  | Sunny    | Mild        | Normal   | Strong | Yes          |\n",
        "| 12  | Overcast | Mild        | High     | Strong | Yes          |\n",
        "| 13  | Overcast | Hot         | Normal   | Weak   | Yes          |\n",
        "| 14  | Rain     | Mild        | High     | Strong | No           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5N-S27OQaM"
      },
      "source": [
        "Το πρώτο βήμα είναι να γράψουμε 4 πίνακες αναφοράς (\"look-up tables\"), έναν για κάθε χαρακτηριστικό,  με την πιθανότητα να παιχτεί ή να μην παιχτεί τέννις σε σχέση με το χαρακτηριστικό. Έχουμε συνολικά 5 περιπτώσεις που δεν μπορέσαμε να παίξουμε και 9 που μπορέσαμε. Οι 4 πίνακες είναι οι ακόλουθοι:\n",
        "\n",
        "| OUTLOOK  | Play = Yes | Play = No | Total |\t| TEMPERATURE | Play = Yes | Play = No | Total |\t| HUMIDITY | Play = Yes | Play = No | Total |\t| WIND   | Play = Yes | Play = No | Total |\n",
        "|----------|------------|-----------|-------|\t|-------------|------------|-----------|-------|\t|----------|------------|-----------|-------|\t|--------|------------|-----------|-------|\n",
        "| Sunny    | 2/9        | 3/5       | 5/14  |\t| Hot         | 2/9        | 2/5       | 4/14  |\t| High     | 3/9        | 4/5       | 7/14  |\t| Strong | 3/9        | 3/5       | 6/14  |\n",
        "| Overcast | 4/9        | 0/5       | 4/14  |\t| Mild        | 4/9        | 2/5       | 6/14  |\t| Normal   | 6/9        | 1/5       | 7/14  |\t| Weak   | 6/9        | 2/5       | 8/14  |\n",
        "| Rain     | 3/9        | 2/5       | 5/14  |\t| Cool        | 3/9        | 1/5       | 4/14  |\t| Cool     | 3/9        | 1/5       | 4/14  |\t| Cool   | 3/9        | 1/5       | 4/14  |\n",
        "και τέλος υπολογίζουμε την πιθανότητα να παίξουμε και να μην παίξουμε:\n",
        "\n",
        "P(Play=Yes) = 9/14\n",
        "\n",
        "P(Play=No) = 5/14\n",
        "\n",
        "#### Inference\n",
        "\n",
        "Έστω ένα νέο δείγμα X = (Outlook=Sunny, Temperature=Cool, Humidity=High, Wind=Strong). Σε ποια κατηγορία ανήκει; (θα παίξουμε τέννις ή όχι).\n",
        "\n",
        "Υπολογίζουμε πρώτα από τους πίνακες αναφοράς την \"πιθανότητα\" να παίξουμε\n",
        "\n",
        "* P(Outlook=Sunny | Play=Yes) = 2/9\n",
        "* P(Temperature=Cool | Play=Yes) = 3/9\n",
        "* P(Humidity=High | Play=Yes) = 3/9\n",
        "* P(Wind=Strong | Play=Yes) = 3/9\n",
        "* P(Play=Yes) = 9/14\n",
        "\n",
        "Σύμφωνα με τον κανόνα ταξινόμησης του NB η πιθανότητα να παίξουμε είναι ανάλογη του γινομένου των προηγούμενων \n",
        "\n",
        "P(X|Play=Yes)P(Play=Yes) = (2/9) \\* (3/9) \\* (3/9) \\* (3/9) \\* (9/14) = 0.0053\n",
        "\n",
        "Υπολογίζουμε παρόμοια την \"πιθανότητα\" να μην παίξουμε\n",
        "\n",
        "* P(Outlook=Sunny | Play=No) = 3/5\n",
        "* P(Temperature=Cool | Play=No) = 1/5\n",
        "* P(Humidity=High | Play=No) = 4/5\n",
        "* P(Wind=Strong | Play=No) = 3/5\n",
        "* P(Play=No) = 5/14\n",
        "\n",
        "P(X|Play=No)P(Play=No) = (3/5) \\* (1/5) \\* (4/5) \\* (3/5) \\* (5/14) = 0.0206\n",
        "\n",
        "Επειδή η ποσότητα 0.0206 είναι μεγαλύτερη από την 0.0053, η απόφαση του Naive Bayes είναι να μην παίξουμε τέννις. Οι ποσότητες αυτές (του αριθμητή) μας αρκούν για την απόφαση γιατί ο παρονομαστής είναι σταθερός. Για να πάρουμε τις πλήρεις πιθανότητες για το συγκεκριμένο δείγμα Χ υπολογίζουμε και τον παρονομαστή:\n",
        "\n",
        "* P(X) = P(Outlook=Sunny) \\* P(Temperature=Cool) \\* P(Humidity=High) \\* P(Wind=Strong)\n",
        "* P(X) = (5/14) \\* (4/14) \\* (7/14) \\* (6/14)\n",
        "* P(X) = 0.02186\n",
        "\n",
        "* P(Play=Yes | X) = 0.0053/0.02186 = 0.2424\n",
        "* P(Play=No | X) = 0.0206/0.02186 = 0.9421"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glTH3pWzOQaP"
      },
      "source": [
        "## Gaussian Naive Bayes (συνεχείς μεταβλητές)\n",
        "\n",
        "Θέλουμε να δοκιμάσουμε τον Naive Bayes στο Wisconsin. Εδώ όμως έχουμε συνεχείς μεταβλητές. Όπως είπαμε θα πρέπει να κάνουμε μια υπόθεση για την κατανομή $P(x_i \\mid y)$. Θα θεωρήσουμε ότι η κατανομή κάθε χαρακτηριστικού ως προς κάθε κλάση ακολουθεί την κανονική κατανομή:\n",
        "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
        "Ο συγκεκριμένος ταξινομητής είναι ο Gaussian Naive Bayes. Πρακτικά, με τα δεδομένα του training set, για κάθε κλάση υπολογίζουμε τη μέση τιμή $\\mu_y$ και τη διακύμανση $\\sigma^2_y$ κάθε χαρακτηριστικού για τη συγκεκριμένη κλάση. \n",
        "\n",
        "Ας δοκιμάσουμε τον Gaussian Naive Bayes στο Wisconsin:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ol6u_RswOQaQ"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "# κάνουμε εκπαίδευση (fit) δηλαδή ουσιαστικά υπολογίζουμε μέση τιμή και διακύμανση για όλα τα χαρακτηριστικά και κλάσεις στο training set\n",
        "model = gnb.fit(train, train_labels)\n",
        "# η GaussianNB έχει builtin μέθοδο υπολογισμό accuracy.\n",
        "gnb.score(test, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "8zR-sydoN4o5"
      },
      "source": [
        "# Παραμετρικοί και μη-παραμετρικοί ταξινομητές, bias - variance trade-off\n",
        "O Gaussian Naive Bayes είναι ένας παραμετρικός ταξινομητής. Οι παραμετρικοί ταξινομητές κάνουν κάποια υπόθεση για την κατανομή (των χαρακτηριστικών) των δεδομένων και την προσδιορίζουν μέσω παραμέτρων. Στην περίπτωση του Gaussian Naive Bayes, η υπόθεση είναι η κανονική κατανομή και οι παράμετροι είναι τα $μ$ και $σ^2$ των χαρακτηριστικών. Αντίθετα, οι μη-παραμετρικές μέθοδοι δεν κάνουν καμμία υπόθεση για την κατανομή των δεδομένων. Προσοχή: και οι μη-παραμετρικοί ταξινομητές έχουν παραμέτρους (και πάρα πολλές σε ορισμένες περιπτώσεις, τα βάρη των νευρωνικών για παράδειγμα) που επηρρεάζουν τη λειτουργία τους αλλά δεν σχετίζονται με κάποια υπόθεση κατανομής για τα δεδομένα. \n",
        "\n",
        "Σε γενικές γραμμές οι παραμετρικοί ταξινομητές είναι απλούστεροι, ταχύτεροι στις φάσεις train/test και χρειάζονται λιγότερα δεδομένα εκπαίδευσης. Από την άλλη, έχουν γενικά μικρότερη χωρητικότητα (capacity), δηλαδή μπορούν να διαχωρίσουν τις κλάσεις σε προβλήματα σχετικά μικρότερων διαστάσεων ενώ η απαίτηση τα πραγματικά δεδομένα να ακολουθούν μια ακριβή κατανομή είναι πολύ ισχυρή και δεν επαληθεύεται πρακτικά. Αντιστρόφως, οι μη παραμετρικοί ταξινομητές είναι πιο αργοί στην εκπαίδευση, έχουν γενικά μεγαλύτερες απαιτήσεις χώρου/μνήμης και χρειάζονται περισσότερα δεδομένα αλλά έχουν μεγαλύτερη χωρητικότητα, μπορούν να μάθουν δυσκολότερα προβλήματα και να έχουν καλύτερη απόδοση σε μεγαλύτερα datasets. \n",
        "\n",
        "Οι μη παραμετρικοί ταξινομητές μπορούν να εμφανίσουν επίσης εντονότερα το πρόβλημα της υπερεκπαίδευσης (overfitting), δηλαδή να προσαρμοστούν υπερβολικά στα δεδομένα εκπαίδευσης και να μειωθεί η ικανότητα γενίκευσης (generalisation) τους σε νέα δείγματα. Γενικά στη φάση της εκπαίδευσης προσπαθούμε να επιτύχουμε μια καλή ισορροπία μεταξυ της απόκλισης (bias) και της διακύμανσης (variance) από τις πραγματικές τιμές.\n",
        "\n",
        "![Bias-Variance trade off](http://www.bogotobogo.com/python/scikit-learn/images/NeuralNetwork7-Overfitting/Overfitting.png \"Bias-Variance trade off\")\n",
        "\n",
        "\n",
        "Ένα παράδειγμα μη-παραμετρικού ταξινομητή που θα εξετάσουμε είναι ο kNN (k-Nearest-Neighbours). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkeJCffkm3Lc"
      },
      "source": [
        "# Διακριτικοί ταξινομητές"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "4ZdyhJaEN4o8"
      },
      "source": [
        "### k Nearest Neighbors Classifier (kNN)\n",
        "O kNN είναι ένας μη παραμετρικός ταξινομητής βασισμένος σε παραδείγματα (instance-based). Η αρχή λειτουργίας του είναι πολύ απλή. Για ένα νέο δείγμα προς ταξινόμηση, πρώτα υπολογίζουμε τους k πλησιέστερους γείτονές του (στον ν-διάστατο χώρο των χαρακτηριστικών εισόδου) με βάση κάποια συνάρτηση απόστασης, συνήθως ευκλείδεια\n",
        "$$d(x, x') = \\sqrt{\\left(x_1 - x'_1 \\right)^2 + \\left(x_2 - x'_2 \\right)^2 + \\dotsc + \\left(x_n - x'_n \\right)^2}$$\n",
        "Η κλάση του νέου δείγματος θα είναι η κλάση της πλειοψηφίας των k γειτόνων (διαλέγουμε k περιττό γενικά), είτε απλά υπολογισμένη (άθροισμα) είτε (αντίστροφα) ζυγισμένη με βάση την απόσταση του κάθε γείτονα. \n",
        "\n",
        "Ο kNN δεν έχει πρακτικά φάση εκπαίδευσης. Ωστόσο, για να ταξινομήσουμε ένα νέο δείγμα στην φάση test,  πρέπει να συγκρίνουμε την απόστασή του με κάθε δείγμα του train set. Αυτό σημαίνει ότι για την ταξινόμηση είναι απαραίτητα όλα τα δείγματα εκπαίδευσης (εξού και η ονομασία \"instance-based\", ενώ στον Naive Bayes χρειαζόμαστε μόνο τις παραμέτρους $μ$ και $σ^2$). Αυτό σημαίνει ότι ο kNN είναι πιο απαιτητικός και σε χώρο (αποθήκευση όλων των δειγμάτων) και σε χρόνο (υπολογισμός όλων των αποστάσεων για κάθε νέο δείγμα).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL0qdum_QFtX"
      },
      "source": [
        "## Υπερπαράμετρος k\n",
        "Το k της γειτονιάς του kNN είναι μια υπερπαράμετρος του ταξινομητή. Μια άλλη υπερπαράμετρος για παράδειγμα είναι η συνάρτηση της απόστασης. Οι υπερπαράμετροι είναι επιλογές που γίνονται από τον σχεδιαστή του συστήματος και δεν μπορούμε να ξέρουμε τις βέλτιστες τιμές τους αν πρώτα δεν τις αξιολογήσουμε εμπειρικά σε δεδομένα.  Ένα άλλο παράδειγμα υπερπαραμέτρου είναι ο αριθμός των κρυφών νευρώνων σε ένα MLP. Στην περίπτωση του kNN το k ελέγχει το trade-off μεταξύ μεταξύ απόκλισης και διακύμνανσης.\n",
        "\n",
        "Έαν θέσουμε μικρό k, πχ k=1 παίρνουμε ένα ταξινομητή με υψηλή διακύμανση και χαμηλή αποκληση. Ο ταξινομητής τείνει να αγνοεί τη συνολική κατανομή και αποφασίζει μόνο από το κοντινότερο δείγμα. Στην περίπτωση k=1 το σύνορο απόφασης (decision boundary) περνά από τις μεσοκάθετους γειτονικών δειγμάτων διαφορετικής κλάσης. \n",
        "\n",
        "![kNN k=1](https://i.stack.imgur.com/UG81y.png \"kNN with k=1\")\n",
        "\n",
        "Αν διαλέξουμε μεγαλύτερο k, φτιάχνουμε ένα ταξινομητή με χαμηλότερη διακύμανση και υψηλότερη απόκλιση. Θα ταξινομίσει λάθος περισσότερα αποκλίνοντα δείγματα (outliers) αλλά θα σέβεται περισσότερο τη συνολική κατανομή.\n",
        "\n",
        "![kNN k=20](https://i.stack.imgur.com/FZITG.png \"kNN with k=20\")\n",
        "\n",
        "Ο kNN ανήκει στην οικογένεια των **ταξινομητών βασισμένων σε στιγμιότυπα (instance based classifiers).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtIbKRgKQLPL"
      },
      "source": [
        "\n",
        "## Το Iris dataset\n",
        "Για να μελετήσουμε τον kNN, θα φορτώσουμε το dataset [\"Iris\"](https://archive.ics.uci.edu/ml/datasets/Iris). Το Iris είναι το διασημότερο dataset στο Machine Learning. Το χρησιμοποίησε πρώτος το 1936 ο διάσημος στατιστικός [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) και αποτελείται από 50 παρατηρήσεις για κάθε είδος του άνθους Iris (Iris setosa, Iris virginica και Iris versicolor). Για κάθε δείγμα μετρήθκαν 4 χαρακτηριστικά: το πλάτος και το μήκος των πετάλων και των σεπάλων.\n",
        "\n",
        "![Iris petals & sepals](https://www.integratedots.com/wp-content/uploads/2019/06/iris_petal-sepal-e1560211020463.png \"Iris petals & sepals\")\n",
        "\n",
        "Σημειώστε ότι με το Iris και τρεις κατηγορίες και άρα έχουμε multiclass και όχι binary classification. Εισάγουμε το Iris στο notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1uirGVkN4o-"
      },
      "source": [
        "# Load Iris and organize our data\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "label_names = data['target_names']\n",
        "labels = data['target']\n",
        "feature_names = data['feature_names']\n",
        "features = data['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSoLp7_UN4pB"
      },
      "source": [
        "# Ας τυπώσουμε τα ονόματα των χαρκτηριστικών\n",
        "print(feature_names)\n",
        "print(label_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIZ_g3-MN4pG"
      },
      "source": [
        "# Χρησιμοποιούμε τη γνωστή train_test_split για να διαχωρίσουμε σε train και test set\n",
        "# το (int) όρισμα \"random_state\" είναι το seed της γεννήτριας τυχαίων αριθμών\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.40, random_state=78)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0D806YqN4pJ"
      },
      "source": [
        "Διαλέγουμε τυχαία τιμή 5 για την υπερπαράμετρο k, εκπαιδεύουμε ένα kNN classifier και υπολογίζουμε την πιστότητα του στο Iris, στο προηγούμενο train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1vVA6_zN4pK"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "pred = knn.predict(X_test)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc4Mx50nN4pO"
      },
      "source": [
        "## Ρύθμιση υπερπαραμέτρων με διασταυρούμενη επικύρωση (Cross Validation)\n",
        "Ένας προφανής τρόπος να βρούμε τη βέλτιστη τιμή του k, να πραγματοποιήσουμε δηλαδή επικύρωση του μοντέλου, είναι ο ακόλουθος. Για k=1 μέχρι k=κάποιο n, κάνουμε fit τον ταξινομητή στο train set και μετράμε την απόδοση στο test set. Ο ταξινομητής με k που δίνει το μικρότερο σφάλμα ταξιμόνόμησης σύμφωνα με κάποιο κριτήριο (εδώ η πιστότητα) στο test set θα είναι ο βέλτιστος. Όμως, αν ακολουθήσουμε αυτή τη στρατηγική, ουσιαστικά κάνουμε υπερεκπαίδευση, καθώς χρησιμοποιούμε το test set ως training set, δηλαδή βελτιστοποιούμε κάποιο κριτήριο σφάλματος πάνω στο test set. Αυτό μπορεί να είναι επιβλαβές για την ικανότητα γενίκευσης του ταξινομητή: το test set χρησιμεύει μόνο για την τελική εκτίμηση της απόδοσης του ταξινομητή.\n",
        "\n",
        "Για να ακολουθήσουμε σωστά το πρωτόκολλο, αυτό που πρέπει να κάνουμε είναι να χρησιμοποιήσουμε μόνο το πραγματικό training set για να διαλέξουμε τις βέλτιστες υπερπαραμέτρους. Θα μπορούσαμε να κρατήσουμε ένα ποσοστό δειγμάτων ως σύνολο επικύρωσης (validation set πχ άλλο ένα 1/3) του training set και να ακολουθήσουμε την προηγούμενη διαδικασία: εκπαίδευση στο 1/3 training set, επικύρωση σε 1/3 και τελικά αξιολόγηση στο 1/3 data set. Ωστόσο αυτή η μεθοδολογία \"αχρηστεύει\" μεγάλο μέρος του dataset (τα 2/3) ως προς την εκπαίδευση του ταξινομητή. Πρακτικά λοιπόν, προτιμούμε να χρησιμοποιούμε τη μέθοδο της διασταυρούμενης επικύρωσης (Cross Validation).\n",
        "\n",
        "Στο Cross Validation αρχικά χωρίζουμε το training set σε έναν αριθμό \"πτυχών\" (folds). Συνηθισμένες τιμές είναι το 5 και το 10 (5-fold και 10-fold CV). Στη συνέχεια, για κάθε k-fold (άσχετο από το k του kNN), θεωρούμε ότι τα k μείον 1 folds είναι training set και ότι το fold που αφήσαμε έξω είναι το test set. Υπολογίζουμε τη μετρική σφάλματός μας στο test set που ορίζει το fold. Επαναλαμβάνουμε τη διαδικασία για τα k folds για κάθε τιμή των υπερπαραμέτρων και υπολογίζουμε τη μέση τιμή της μετρικής του σφάλματος. Με αυτό τον τρόπο, αφενός είμαστε αμερόληπτοι στην αξιολόγηση αφήνοντας τελείως έξω το test set και αφετέρου χρησιμοποιούμε αποτελεσματικά τα δεδομένα εκπαίδευσης: τα χρησιμοποιούμε όλα και παίρνοντας τη μέση τιμή εξαλείφουμε πιθανές ανωμαλίες στα δεδομένα.\n",
        "\n",
        "![Cross validation](https://sebastianraschka.com/images/faq/evaluate-a-model/k-fold.png \"Cross Validation\")\n",
        "\n",
        "To Scikit Learn έχει συναρτήσεις για να κάνει αυτόματα cross validation (να ορίζει folds και να  υπολογίζει τιμές και μέσους όρους). Θα κάνουμε 5 fold cross validation για να υπολογίσουμε το βέλτιστο k του kNN στο Iris."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFEqoygIPQ2i"
      },
      "source": [
        "# Ταξινομητές βασισμένοι στην παλινδόρμηση: Λογιστική Παλινδρόμηση (Logistic Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNkEKo8qLYSo"
      },
      "source": [
        "Παρόμοια με τον Αφελή Μπεϋζιανό, θα φτιάξουμε έναν ταξινομητή βασισμένο στη Λογιστική Παλινδόμηση from scratch, προτού χρησιμοποιήσουμε την έτοιμη συνάρτηση του scikit. Ξεκινάμε, όπως είναι αναμενόμενο για αυτό το παράδειγμα ταξινομητών, από την γραμμική παλινδόμηση."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWoh55HtNCRo"
      },
      "source": [
        "## Γραμμική παλινδρόμηση\n",
        "\n",
        "Η συνάρτηση της ευθείας είναι $y=mx+b$, όπου $m$ η κλίση της ευθείας και $b$ το σημείο τομής του άξονα $Y$. Στην περίπτωση όπου τα δεδομένα έχουν μία διάσταση (scalar), η διαχωριστική επιφάνεια (*η υπόθεση*) του ταξινομητή είναι μια απλή ευθεία, σύμφωνα με τον προηγούμενο τύπο. Η εύρεση των *εμπειρικών* $\\hat{m}$ και $\\hat{b}$ γίνεται με τη μέθοδο των ελαχίστων τετραγώνων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIdm6eJfiMIP"
      },
      "source": [
        "### Μέθοδος των Ελάχιστων Τετραγώνων\n",
        "Με τη Μέθοδο των Ελαχίστων Τετραγώνων προσπαθούμε να βρούμε μια ευθεία για την οποία η απόσταση κάθε σημείου $\\{x_i, y_i\\}$ είναι η ελάχιστη. Δήλαδή θέλουμε να βρούμε $$\\min_{a,m}Q(a,m) \\mbox{ όπου } Q(b,m) = \\sum_{i=1}^n\\hat{\\varepsilon}_i^{\\,2} = \\sum_{i=1}^n (y_i - b - m x_i)^2\\ $$\n",
        "\n",
        "Χρησιμοποιώντας απειροστικό λογισμό, την γεωμετρία του εσωτερικού γινόμενου ή απλά αναπτύσσοντας την συνάρτηση μπορεί να δειχθεί ότι οι τιμές $ b $ και $ m $ οι οποίες ελαχιστοποιούν την συνάρτηση $Q(b,m)$ είναι:\n",
        "$$ \\begin{align} \\hat{m} & = \\frac{ \\sum_{i=1}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y}) }{ \\sum_{i=1}^{n} (x_{i}-\\bar{x})^2 }\n",
        "  = \\frac{ \\sum_{i=1}^{n}{x_{i}y_{i}} - \\frac1n \\sum_{i=1}^{n}{x_{i}}\\sum_{j=1}^{n}{y_{j}}}{ \\sum_{i=1}^{n}({x_{i}^2}) - \\frac1n (\\sum_{i=1}^{n}{x_{i}})^2 } =\\\\  &= \\frac{ \\overline{xy} - \\bar{x}\\bar{y} }{ \\overline{x^2} - \\bar{x}^2 } =   \\frac{ \\operatorname{Cov}[x,y] }{ \\operatorname{Var}[x] } = r_{xy} \\frac{s_y}{s_x},\\\\ \\hat{b}  & = \\bar{y} - \\hat{m}\\,\\bar{x}\\end{align}$$\n",
        "\n",
        "όπου $r_{xy}$ είναι μια παράμετρος συσχέτισης μεταξύ $x$ και $y$, $s_x$ είναι η τυπική απόκλιση του $x$, και $s_y$ είναι αντίστοιχα η τυπική απόκλιση του $y$. Η οριζόντια γραμμή πάνω από μια μεταβλητή δηλώνει τον απλό μέσο όρο της μεταβλητής. Για παράδειγμα: $\\overline{xy} = \\tfrac{1}{n}\\textstyle\\sum_{i=1}^n x_iy_i.$ Τα \"b καπέλο\" $\\hat{b}$ και \"m καπέλο\" $\\hat{m}$ ονομάζονται εκτιμήτριες ελάχιστων τετραγώνων.\n",
        "Αντικαθιστώντας τις παραπάνω μαθηματικές εκφράσεις για τις παραμέτρους $\\hat{b}$ και $\\hat{m}$ στο $ y = \\hat{b} + \\hat{m} x, \\,$ πέρνουμε\n",
        "\n",
        "$\\frac{ y-\\bar{y}}{s_y} = r_{xy} \\frac{ x-\\bar{x}}{s_x}  $\n",
        "\n",
        "Αυτό δείχνει ότι το $r_{xy}$ έχει το ρόλο της γραμμής παλινδρόμησης για τα σημεία. Η συνάρτηση $ y = \\hat{b} + \\hat{m} x, $ λέγεται ευθεία ελαχίστων τετραγώνων ή ευθεία παλινδρόμησης. \n",
        "\n",
        "Στη μηχανική μάθηση η συνάρτηση αυτή λέγεται συνάρτηση υπόθεσης ή απλά *υπόθεση* και συμβολίζεται ως $ h_\\theta(x) = \\theta_0 + \\theta_1x$ (το $\\hat{b}$ και $\\hat{m}$ είναι οι παράμετροι $\\theta_0$ και $\\theta_1$ αντίστοιχα).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG6s1bzFsLO9"
      },
      "source": [
        "### Πολλαπλή γραμμική παλινδρόμηση\n",
        "\n",
        "Σε περίπτωση που έχουμε δεδομένα που βρίσκονται εντός ενός πολυδιάστατου χώρου, κάτι σύνηθες σε προβλήματα μηχανικής μάθησης, η ευθεία γενικεύεται (για *διανύσματα* εισόδου πλέον) σε $$y= β_1 ​ x_1 ​+...+β_nx_n ​ +b,$$ και μιλάμε πλέον για το πρόβλημα της πολλαπλής γραμμικής παλινδρόμησης όπου αναζητούμε τις τιμές $β$ (διάνυσμα) και b που ορίζουν το υπερεπίπεδο που έχει τη βέλτιστη προσαρμογή στα δεδομένα. \n",
        "\n",
        "Σημειώστε ότι το $y$ παραμένει ένας βαθμωτός αριθμος o οποίος στη γενική περίπτωση μπορεί να πάρει οποιαδήποτε τιμή."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRRMORlYzzKv"
      },
      "source": [
        "## Από τη γραμμική στη λογιστική παλινδρόμηση\n",
        "\n",
        "Σε περίπτωση που θα θέλαμε να μετατρέψουμε το πρόβλημα της παλινδρόμησης σε ένα πρόλβημα δυαδικής ταξινόμησης μια προσέγγιση θα ήταν να κοιτάξουμε το πεδίο τιμών της $y$ και να θέσουμε μια τιμή κατωφλιού $y_{thresh}$ για την οποία θα ισχύει $$\\begin{align} y_{i} &=1 \\text{, αν } y_i \\geq y_{thresh} \\text{ και }\\\\\n",
        "y_{i} &=0 \\text{ αλλιώς} \\end{align}\n",
        "$$\n",
        "Το πρόβλημα με αυτή την ευρετική μέθοδο είναι ότι ενώ πρακτικά πράγματι μετατρέπει το πρόβλημα παλινδρόμησης σε δυαδική ταξινόμηση, δεν υπάρχει κάποια θεωρητική βάση για να υποστηρίξει την ορθότητα της απόφασης μας. Αυτή θα υπήρχε, αν με κάποιο τρόπο μπορούσαμε να λάβουμε μια συνάρτηση πυκνότητας πιθανότητας για τα $P(y_i)$ έτσι ώστε αν $P(y_i)\\geq 0,5$ να έχουμε $y_{i} =1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veDrcRz2-Zyr"
      },
      "source": [
        "### Η λογιστική (σιγμοειδής) συνάρτηση\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
        "\n",
        "Μια ιδιαίτερα βολική συνάρτηση για να λάβουμε τιμές στο διάσημα μεταξύ 0 και 1 είναι λογιστική σιγμοειδής συνάρτηση ή απλά λογιστική συνάρτηση:\n",
        "$$ f(x)={\\frac {1}{1+e^{-x}}} .$$ \n",
        "Ας την ορίσουμε:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukEFhHOuAakl"
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x: float) -> float:\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "assert sigmoid(0) == 0.5 \n",
        "# Με την assert κάνουμε έλεγχο τιμών, μια καλή πρακτική \n",
        "# Αν δεν προκύπτει η ισότητα ο διερμηνευτής της Python θα πετάξει σφάλμα. \n",
        "# Αν είναι σωστή η τιμή η εκτέλεση θα συνεχιστεί χωρίς κάποια έξοδο."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg_kEX4LCLql"
      },
      "source": [
        "### Δημιουργία μιας συνάρτησης καταστολής (squashing function)\n",
        "\n",
        "Καταρχάς θα χρησιμοποιήσουμε ένα μικρό αλγεβρικό \"τρικ\" για να \"ξεφορτωθούμε\" τον όρο $b$ και να πάρουμε τη βολική μορφή του γινομένου. Θα προσθέσουμε στην αρχή του διανύσματος του $x$ έναν άσσο και στην αρχή του διανύσματος $β$ το $b$ ως εξής:\n",
        "$$\n",
        "\\vec{x}=\\left(\\begin{array}{c}\n",
        "1 \\\\\n",
        "x_{1} \\\\\n",
        "\\cdots \\\\\n",
        "x_{n}\n",
        "\\end{array}\\right) \\vec { β }=\\left(\\begin{array}{c}\n",
        "b \\\\\n",
        "\\beta_{1} \\\\\n",
        "\\cdots \\\\\n",
        "\\beta_{n}\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "Συνεπώς τώρα μπορούμε να γράψουμε\n",
        "$$y= β_1 ​ x_1 ​+...+β_nx_n ​ +b= b +β_1 ​ x_1 ​+...+β_nx_n ​ =\\vec{x}\\cdot \\vec{β}$$\n",
        "όπου \"$\\cdot$\" είναι το εσωτερικό γινόμενο των δύο διανυσμάτων. Υλοποιούμε το εσωτερικό γινόμενο δύο διανυσμάτων:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivETpwcMFp7t"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def dot(a: List[float], b: List[float]) -> float:\n",
        "    assert len(a) == len(b)\n",
        "    return sum([a_i * b_i for a_i, b_i in zip(a, b)])\n",
        "\n",
        "assert dot([1, 2, 3, 4], [5, 6, 7, 8]) == 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpNO6AVvGcPp"
      },
      "source": [
        "Μπορούμε τώρα να γράψουμε τώρα τη δική μας συνάρτηση καταστολής `squash` η οποία εφαρμόζει τη σιγμοειδή συνάρτηση πάνω στο εσωτερικό γινόμενο δύο διανυσμάτων ή αλλιώς που υπολογίζει το \n",
        "$$ y= \\sigma(β_1 ​ x_1 ​+...+β_nx_n ​ +b)$$ όπου $\\sigma$ η σιγμοειδής:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHm8B3w0HIip"
      },
      "source": [
        "def squash(beta: List[float], x: List[float]) -> float:\n",
        "    assert len(beta) == len(x)\n",
        "    # Calculate the dot product\n",
        "    dot_result: float = dot(beta, x)\n",
        "    # Use sigmoid to get a result between 0 and 1\n",
        "    return sigmoid(dot_result)\n",
        "\n",
        "assert squash([1, 2, 3, 4], [5, 6, 7, 8]) == 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iDFs1TxH9lm"
      },
      "source": [
        "Έχουμε λοιπόν μετατρέψει το πρόβλημα παλινδρόμησης σε πρόβλημα δυαδικής ταξινόμησης. Το ζητούμενό μας τώρα είναι το εξής: εφόσον μιλάμε για ταξινόμηση, δηλαδή επιβλεπόμενη μάθηση, για κάθε $x_i$ θα μας δίνονται οι επιθυμητές ετικέτες 0 και 1 $y_i$ και εμείς θα πρέπει να προσδιορίσουμε τις βέλτιστες τιμές του $β$ ή αλλοιώς να προσδιορίσουμε την υπόθεση μας με όρους μηχανικής μάθησης."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1kBp94UJ_zX"
      },
      "source": [
        "### Συνάρτηση πυκνότητας πιθανότητας και συνάρτηση κόστους\n",
        "\n",
        "Για να μπορέσουμε να προχωρήσουμε θα πρέπει να ορίσουμε μια συνάρτηση υπό συνθήκης πιθανότητας που θα εκφράζει κατά πόσο \"πλησιάζουμε\" στη σωστή ετικέτα $y_i$ για κάθε δείγμα $x_i$ και τα βάρη μας $β$. Αυτή η συνάρτηση θα πρέπει αν έχει πιθανότητα $\\sigma(βx_i)$ για να είναι η ετικέτα \"1\" και $1-\\sigma(βx_i)$ για να είναι η ετικέτα \"0\". Μπορούμε να το γράψουμε σε μία εξίσωση ως εξής:\n",
        "$$\n",
        "P\\left(y_{i} \\mid \\beta x_{i}\\right)=\\sigma\\left(\\beta x_{i}\\right)^{y_{i}} \\times\\left(1-\\sigma\\left(\\beta x_{i}\\right)\\right)^{1-y_{i}}\n",
        "$$\n",
        "όπου ανάλογα αν $y_i=1$ ή $y_i=0$ μένει μόνο ο πρώτος ή ο δεύτερος όρος στη δεξιά πλευρά της εξίσωσης. Πρόκειται για τη συνάρτηση πιθανοφάνειας της υπόθεσής μας, η οποία επιθυμούμε να είναι η μέγιστη δυνατή.\n",
        "\n",
        "Εφόσον μας δίνονται τα $x_i$ και $y_i$ και πρέπει να βρούμε τα βέλτιστα βάρη $β$ θα χρειαστούμε μια συνάρτηση σφάλματος ή απώλειας (loss) $L(β|y_ix_i)$ προς ελαχιστοποίηση. \n",
        "Αρχικά, θα πάρουμε τον λογάριθμο της πιθανοφάνειας, κάτι που κανουμε συχνά για υπολογιστικούς λόγους. Ο λογάριθμος έχει την ίδια μονοτονία με την αρχικη συνάρτηση αλλά μας δίνει αθροίσματα τιμών αντί για γινόμενα. Στη συνέχεια, εφόσον στοχεύουμε σε ελαχιστοποίηση, θα χρησιμοποιήσουμε την αρνητική συνάρτηση του λογάριθμου της log-πιθανοφάνειας. Τελικά προκύπτει:\n",
        "$$\n",
        "\\log L\\left(\\beta \\mid x_{i} y_{i}\\right)=-\\left(y_{i} \\log \\left(\\sigma\\left(\\beta x_{i}\\right)\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\sigma\\left(\\beta x_{i}\\right)\\right)\\right)\n",
        "$$\n",
        "που είναι η συνάρτηση λογαριθμικού σφάλματος ή απώλειας (Log Loss / Log Likelihood) προς ελαχιστοποίηση. Την ορίζουμε:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBnD8-7JTUNc"
      },
      "source": [
        "def neg_log_likelihood(y: float, y_pred: float) -> float:\n",
        "    return -((y * math.log(y_pred)) + ((1 - y) * math.log(1 - y_pred)))\n",
        "\n",
        "assert 2.30 < neg_log_likelihood(1, 0.1) < 2.31\n",
        "assert 2.30 < neg_log_likelihood(0, 0.9) < 2.31\n",
        "assert 0.10 < neg_log_likelihood(1, 0.9) < 0.11\n",
        "assert 0.10 < neg_log_likelihood(0, 0.1) < 0.11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu3_kRq9UiDq"
      },
      "source": [
        "### Συνάρτηση κόστους\n",
        "Η συνάρτηση κόστους (cost function) είναι απλά το άθροισμα του σφάλματος για όλο το dataset: \n",
        "$$\n",
        "\\text { Cost }=-\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i} \\log \\left(\\sigma\\left(\\beta x_{i}\\right)\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\sigma\\left(\\beta x_{i}\\right)\\right)\\right)\n",
        "$$\n",
        "Την ορίζουμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgeXUxP2U4Bl"
      },
      "source": [
        "def error(ys: List[float], ys_pred: List[float]) -> float:\n",
        "    assert len(ys) == len(ys_pred)\n",
        "    num_items: int = len(ys)\n",
        "    sum_nll: float = sum([neg_log_likelihood(y, y_pred) for y, y_pred in zip(ys, ys_pred)])\n",
        "    return (1 / num_items) * sum_nll\n",
        "\n",
        "assert 2.30 < error([1], [0.1]) < 2.31\n",
        "assert 2.30 < error([0], [0.9]) < 2.31\n",
        "assert 0.10 < error([1], [0.9]) < 0.11\n",
        "assert 0.10 < error([0], [0.1]) < 0.11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTcGv43CV3Vo"
      },
      "source": [
        "### Βελτιστοποίηση με κάθοδο κλίσης\n",
        "\n",
        "Η μέθοδος βελτιστοποίησης που χρησιμοποιούμε κατά κανόνα στη μηχανική μάθηση είναι η κάθοδος κλίσης (gradient descent),\n",
        "$$\n",
        "\\frac{\\partial \\text { cost }}{\\partial x_{i j}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\sigma\\left(\\beta x_{i}\\right)-y_{i}\\right) x_{i j},\n",
        "$$\n",
        "η οποία εγγυάται την εύρεση ενός τοπικού ελάχιστου. Προγραμματιστικά αυτό μπορεί να γραφτεί ως εξής:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EBX-wGXNOa"
      },
      "source": [
        "```python\n",
        "grad: List[float] = [0 for _ in range(len(beta))]\n",
        "for x, y in zip(xs, ys):\n",
        "    err: float = squash(beta, x) - y\n",
        "    for i, x_i in enumerate(x):\n",
        "        grad[i] += (err * x_i)\n",
        "grad = [1 / len(x) * g_i for g_i in grad]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnz1cORqbWZI"
      },
      "source": [
        "## Ένα παράδειγμα\n",
        "\n",
        "Θα δοκιμάσουμε τον ταξινομήτή που φτιάξαμε σε ένα παράδειγμα με ένα δiσδιάστατο διαχωρίσιμο dataset.\n",
        "Κατεβάζουμε τα δεδομένα μας και τα διαβάζουμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHMxtGWnb6bb"
      },
      "source": [
        "!wget -nc  https://tinyurl.com/y5oyomko -O data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldkPtbR8cFzL"
      },
      "source": [
        "toy_data_file = 'data.csv'\n",
        "\n",
        "xs: List[List[float]] = []\n",
        "ys: List[float] = []\n",
        "\n",
        "with open(toy_data_file) as file:\n",
        "    for line in file:\n",
        "        data_point: List[str] = line.strip().split(',')\n",
        "        y: int = int(data_point[0])\n",
        "        x1: float = float(data_point[1])\n",
        "        x2: float = float(data_point[2])\n",
        "        xs.append([x1, x2])\n",
        "        ys.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9W8EcwTi1od"
      },
      "source": [
        "Ας κάνουμε μια απεικόνιση των δεδομένων μας:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTZ7U24MejvE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1s: List[float] = [x[0] for x in xs]\n",
        "x2s: List[float] = [x[1] for x in xs]\n",
        "\n",
        "plt.scatter(x1s, x2s, c=ys)\n",
        "plt.axis([min(x1s), max(x1s), min(x2s), max(x2s)]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRoolX2ri63w"
      },
      "source": [
        "Για να μπορέσουμε να χρησιμοποιήσουμε το αλγεβρικό τρικ που μας δίνει το γινόμενο $βx$ πρέπει να προσθέσουμε μια μονάδα στην αρχή κάθε διανύσματος εισόδου:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyW3TLUtjKKR"
      },
      "source": [
        "for x in xs:\n",
        "    x.insert(0, 1)\n",
        "\n",
        "xs[:5] # για να δούμε τα πέντε πρώτα διανύσματα\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsPmBp6KkWsw"
      },
      "source": [
        "Έχουμε πλέον ό,τι χρειαζόμαστε για να προσαρμόσουμε τον ταξινομητή μας στα διαθέσιμα δεδομένα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0o-l_28jmoj"
      },
      "source": [
        "import random\n",
        "\n",
        "beta: List[float] = [random.random() for _ in range(3)]\n",
        "\n",
        "print(f'Starting with \"beta\": {beta}')\n",
        "\n",
        "epochs: int = 5000\n",
        "learning_rate: float = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Calculate the \"predictions\" (squishified dot product of `beta` and `x`) based on our current `beta` vector\n",
        "    ys_pred: List[float] = [squash(beta, x) for x in xs]\n",
        "\n",
        "    # Calculate and print the error\n",
        "    if epoch % 1000 == True:\n",
        "        loss: float = error(ys, ys_pred)\n",
        "        print(f'Epoch {epoch} --> loss: {loss}')\n",
        "\n",
        "    # Calculate the gradient\n",
        "    grad: List[float] = [0 for _ in range(len(beta))]\n",
        "    for x, y in zip(xs, ys):\n",
        "        err: float = squash(beta, x) - y\n",
        "        for i, x_i in enumerate(x):\n",
        "            grad[i] += (err * x_i)\n",
        "    grad = [1 / len(x) * g_i for g_i in grad]\n",
        "\n",
        "    # Take a small step in the direction of greatest decrease\n",
        "    beta = [b + (gb * -learning_rate) for b, gb in zip(beta, grad)]\n",
        "\n",
        "print(f'Best estimate for \"beta\": {beta}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLaCNWP2klt8"
      },
      "source": [
        "Για να εκτιμήσουμε την απόδοση του ταξινομήτη μας θα τυπώσουμε τα γνωστά στατιστικά που συγκροτούν τον πίνακα σύγχυσης καθώς και τη συνολική ορθότητα.\n",
        "\n",
        "Σημειώστε ότι τώρα το να θέσουμε ένα κατώφλι $\\geq 0,5$ για την απόφασή μας είναι δόκιμο καθώς η έξοδος του ταξινομητή μας είναι μια συνάρτηση πυκνότητας πιθανότητας."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTnb3DHwlQWu"
      },
      "source": [
        "total: int = len(ys)\n",
        "thresh: float = 0.5\n",
        "true_positives: int = 0\n",
        "true_negatives: int = 0\n",
        "false_positives: int = 0\n",
        "false_negatives: int = 0\n",
        "for i, x in enumerate(xs):\n",
        "    y: int = ys[i]\n",
        "    pred: float = squash(beta, x)\n",
        "    y_pred: int = 1\n",
        "    if pred < thresh:\n",
        "        y_pred = 0\n",
        "    if y == 1 and y_pred == 1:\n",
        "        true_positives += 1\n",
        "    elif y == 0 and y_pred == 0:\n",
        "        true_negatives += 1\n",
        "    elif y == 1 and y_pred == 0:\n",
        "        false_negatives += 1\n",
        "    elif y == 0 and y_pred == 1:\n",
        "        false_positives += 1\n",
        "\n",
        "print(f'True Positives: {true_positives}')\n",
        "print(f'True Negatives: {true_negatives}')\n",
        "print(f'False Positives: {false_positives}')\n",
        "print(f'False Negatives: {false_negatives}')\n",
        "print(f'Accuracy: {(true_positives + true_negatives) / total}')\n",
        "print(f'Error rate: {(false_positives + false_negatives) / total}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXlme8fZlg6U"
      },
      "source": [
        "Τέλος, τυπώνουμε την ευθεία διαχωρισμού:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOCKeJqqlnHg"
      },
      "source": [
        "x1s: List[float] = [x[1] for x in xs]\n",
        "x2s: List[float] = [x[2] for x in xs]\n",
        "plt.scatter(x1s, x2s, c=ys)\n",
        "plt.axis([min(x1s), max(x1s), min(x2s), max(x2s)]);\n",
        "\n",
        "m: float = -(beta[1] / beta[2])\n",
        "b: float = -(beta[0] / beta[2])\n",
        "\n",
        "x2s: List[float] = [m * x[1] + b for x in xs]\n",
        "\n",
        "plt.plot(x1s, x2s, '--');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTKO53NDluvf"
      },
      "source": [
        "## Λογιστική παλινδρόμηση του scikit με πολλές κατηγορίες"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-yzcdzl2-sT"
      },
      "source": [
        "### Loading the Data (Digits Dataset)\n",
        "The digits dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below will load the digits dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6jdIMT73JVH"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G53Lm2153KCF"
      },
      "source": [
        "Now that you have the dataset loaded you can use the commands below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7JnxzLZ3Og8"
      },
      "source": [
        "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
        "print(\"Image Data Shape\" , digits.data.shape)\n",
        "# Print to show there are 1797 labels (integers from 0–9)\n",
        "print(\"Label Data Shape\", digits.target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdSKx1p03QkG"
      },
      "source": [
        "to see that there are 1797 images and 1797 labels in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymsA3jSB3aG1"
      },
      "source": [
        "### Showing the Images and the Labels (Digits Dataset)\n",
        "This section is really just to show what the images and labels look like. It usually helps to visualize your data to see what you are working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhpEVqIN3n4n"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,4))\n",
        "for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n",
        " plt.subplot(1, 5, index + 1)\n",
        " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
        " plt.title('Training: %i\\n' % label, fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIT5y3JU3oa8"
      },
      "source": [
        "### Splitting Data into Training and Test Sets (Digits Dataset)\n",
        "\n",
        "We make training and test sets to make sure that after we train our classification algorithm, it is able to generalize well to new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdAnpkLn3vTC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fEptG_a4Ru4"
      },
      "source": [
        "### Train a Logistic Regression classifier and make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2wh1D66_ZK"
      },
      "source": [
        "[Documentation απο το scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yocL-IL3xZk"
      },
      "source": [
        "# Step 1. Import the model you want to use\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Step 2. Make an instance of the Model\n",
        "# all parameters not specified are set to their defaults\n",
        "logisticRegr = LogisticRegression(solver='saga', max_iter=500)\n",
        "# Step 3. Training the model on the data, storing the information learned from the data\n",
        "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
        "logisticRegr.fit(x_train, y_train)\n",
        "# Step 4. Predict labels for new data (new images)\n",
        "# Uses the information the model learned during the model training process\n",
        "# Returns a NumPy Array\n",
        "# Predict for One Observation (image)\n",
        "logisticRegr.predict(x_test[0].reshape(1,-1))\n",
        "#Predict for Multiple Observations (images) at Once\n",
        "logisticRegr.predict(x_test[0:10])\n",
        "#Make predictions on entire test data\n",
        "predictions = logisticRegr.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIam0WBF4XZH"
      },
      "source": [
        "### Measuring Model Performance (Digits Dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC3AHHf5mJer"
      },
      "source": [
        "\n",
        "#### Accuracy\n",
        "\n",
        "To see how the model performs on the new data (test set) we will use the accuracy metric. \n",
        "Accuracy is defined as: \n",
        "(fraction of correct predictions): correct predictions / total number of data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJauWYyT47jO"
      },
      "source": [
        "# Use score method to get accuracy of model\n",
        "score = logisticRegr.score(x_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RNeVK5I49fL"
      },
      "source": [
        "#### Confusion Matrix \n",
        "\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known.  \n",
        "\n",
        "Text only version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2giouC95MXS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, predictions)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFHQG3UV5S31"
      },
      "source": [
        "Using Seaborn: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT_chryr5e3u"
      },
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8uNhuWQmnrz"
      },
      "source": [
        "# Άσκηση: kNN from scratch\n",
        "\n",
        "Χρησιμοποιώντας το υλικό αυτό του notebook μπορείτε να φτιάξετε έναν ταξινομητή kNN from scratch και να τον αξιολογήσετε στο toy data set ή στο Wisconsin Breast Cancer ή στο Iris;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNkh1aPm5JD"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import operator\n",
        "\n",
        "# function to calculate euclidean distance\n",
        "def euc_dist(x1, x2):\n",
        "    return np.sqrt(np.sum((x1-x2)**2))\n",
        "\n",
        "class myKNearestNeighbors():\n",
        "    \n",
        "    def __init__(self, K):\n",
        "        self.K = K\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        self.X_train = x_train\n",
        "        self.Y_train = y_train\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \n",
        "        # list to store all our predictions\n",
        "        predictions = []\n",
        "        \n",
        "        # loop over all observations\n",
        "        for i in range(len(X_test)):            \n",
        "            \n",
        "            # calculate the distance between the test point and all other points in the training set\n",
        "            dist = np.array([euc_dist(X_test[i], x_t) for x_t in self.X_train])\n",
        "            \n",
        "            # sort the distances and return the indices of K neighbors\n",
        "            dist_sorted = dist.argsort()[:self.K]\n",
        "            \n",
        "            # get the neighbors\n",
        "            neigh_count = {}\n",
        "\n",
        "            # for each neighbor find the class\n",
        "            for idx in dist_sorted:\n",
        "                if self.Y_train[idx] in neigh_count:\n",
        "                    neigh_count[self.Y_train[idx]] += 1\n",
        "                else:\n",
        "                    neigh_count[self.Y_train[idx]] = 1\n",
        "            \n",
        "            # get the most common class label \n",
        "            sorted_neigh_count = sorted(neigh_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "            \n",
        "            predictions.append(sorted_neigh_count[0][0])\n",
        "        return predictions\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "data = iris.data    \n",
        "target = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=5656)\n",
        "\n",
        "clf = myKNearestNeighbors(K=3)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}