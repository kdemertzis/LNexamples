{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3PDiheAL7myjZ3PcTlcmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdemertzis/LNexamples/blob/main/DF_autokeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZL-i0YQSbDA",
        "outputId": "510d2e39-e79c-43cb-8dad-646b50bd45cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from torchdiffeq) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from torchdiffeq) (2.0.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.7.4.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (10.9.0.58)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.7.99)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (2.14.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.7.101)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (8.5.0.96)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (11.10.3.66)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->torchdiffeq) (63.4.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->torchdiffeq) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsH_kyE_R5V-",
        "outputId": "6ef4b789-40da-4de9-f559-1a62dd18267c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Collecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.0)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.4.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
            "Collecting lit\n",
            "  Downloading lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=90003 sha256=efa740cb41e7f7052e17851dfd51b1be518e9c68206339379da44fd3f3677bf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/68/18/2ad49b416abb9139c8217c349fd9df0674da8f0d1952db2ea5\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-15.0.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mNGxuZUaRGIR",
        "outputId": "536973bf-2252-4fae-d118-69eb120b85d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aee0d50fee82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Solve the differential equation using automatic differentiation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdamped_vibration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqi_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'odeint'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Define the input parameters\n",
        "E = 29000000.0 # Modulus of elasticity (kN/m2)\n",
        "I = 0.00423 # Moment of inertia of the cross-section (m4)\n",
        "Area = 0.69 # Area of the cross-section (m2)\n",
        "L = 4.0 # Length of beam (m)\n",
        "p = 27395.515 # Mass/volume unit (kg/m3)\n",
        "kf = 1500.0 # Modulus of the subgrafe reaction (kN/m2)\n",
        "Pe = 500.0 # External moving load (kN)\n",
        "c = 1.00 # Velocity of the external load (m/sec)\n",
        "\n",
        "# A. STEP: Solution of eigenvalue problem -> Calculation of eigenfrequencies and\n",
        "# eigenvectors: EI*(d4X/dx4)+(kf-wn2*p*Area)*X=0\n",
        "# First eigenfrequency\n",
        "n = 1\n",
        "wn = np.sqrt((1/(p*Area))*((((n**4)*(np.pi**4)*E*I)/(L**4))+kf))\n",
        "# First eigenvector - value in the middle of the beam : x=L/2\n",
        "Pn = ((wn**2*p*Area-kf)/(E*I))**(1/4)\n",
        "X1L2 = np.sin(Pn*(L/2))\n",
        "\n",
        "# B. STEP: Calculation of the generalized parameters qi(t)\n",
        "# (dqi2/dt2)+((wn2*Mn+kf*Nn)/Mn)*qi=(Pe/Mn)*sin(Pn*c*t)\n",
        "wn2 = wn**2\n",
        "Nn = (L/2)-(np.sin(2*Pn*L)/(4*Pn))\n",
        "Mn = p*Area*Nn\n",
        "k = Pe/Mn\n",
        "beta = (wn2*Mn+kf*Nn)/Mn\n",
        "\n",
        "# Define the initial state of the system\n",
        "qi_init = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "# Define the function that describes the dynamics of the system\n",
        "def damped_vibration(qi, t):\n",
        "    q1, q2 = qi\n",
        "    dq1dt = q2\n",
        "    dq2dt = k*torch.sin(Pn*c*t) - beta*q1\n",
        "    return torch.tensor([dq1dt, dq2dt])\n",
        "\n",
        "# Define the time steps for the simulation\n",
        "t = torch.linspace(0, 8.0, 801)\n",
        "\n",
        "# Solve the differential equation using automatic differentiation\n",
        "q = torch.odeint(damped_vibration, qi_init, t)\n",
        "q1 = q[:, 0]\n",
        "\n",
        "# Train-test split the data\n",
        "A1mat = np.arange(0, 8.01, 0.01)\n",
        "A2mat = X1L2*q1*100\n",
        "X_train, X_test, y_train, y_test = train_test_split(A1mat.reshape(-1, 1), A2mat.reshape(-1), test_size=0.15, random_state=42)\n",
        "\n",
        "# Convert the training and testing data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network model\n",
        "class Net(torch.nn.Module):\n",
        "  def init(self):\n",
        "      super(Net, self).init()\n",
        "      self.fc1 = torch.nn.Linear(1, 64)\n",
        "      self.fc2 = torch.nn.Linear(64, 64)\n",
        "      self.fc3 = torch.nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = torch.nn.functional.relu(self.fc1(x))\n",
        "      x = torch.nn.functional.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "# Instantiate the neural network model and define the loss function\n",
        "model = Net()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1000):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)\n",
        "\n",
        "# Compute the loss\n",
        "loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "# Backward pass and optimization step\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# Print the loss every 100 epochs\n",
        "if epoch % 100 == 0:\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Compute the loss\n",
        "loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "# Backward pass and optimization step\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# Print the loss every 100 epochs\n",
        "if epoch % 100 == 0:\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#Evaluate the model on the testing data\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    mse_test = mean_squared_error(y_test.numpy(), y_pred.numpy())\n",
        "    mae = mean_absolute_error(y_test.numpy(), y_pred.numpy())\n",
        "    rmse = np.sqrt(mse_test)\n",
        "    r2 = r2_score(y_test.numpy(), y_pred.numpy())\n",
        "\n",
        "#Print the results\n",
        "print('\\nNeural Network Performance Metrics:')\n",
        "print('Test Set Mean Squared Error:', mse_test)\n",
        "print('Mean Absolute Error:', mae)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "print('Coefficient of Determination (R^2):', r2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from torchdiffeq import odeint\n",
        "\n",
        "# Define the input parameters\n",
        "E = 29000000.0 # Modulus of elasticity (kN/m2)\n",
        "I = 0.00423 # Moment of inertia of the cross-section (m4)\n",
        "Area = 0.69 # Area of the cross-section (m2)\n",
        "L = 4.0 # Length of beam (m)\n",
        "p = 27395.515 # Mass/volume unit (kg/m3)\n",
        "kf = 1500.0 # Modulus of the subgrafe reaction (kN/m2)\n",
        "Pe = 500.0 # External moving load (kN)\n",
        "c = 1.00 # Velocity of the external load (m/sec)\n",
        "\n",
        "# A. STEP: Solution of eigenvalue problem -> Calculation of eigenfrequencies and\n",
        "# eigenvectors: EI*(d4X/dx4)+(kf-wn2*p*Area)*X=0\n",
        "# First eigenfrequency\n",
        "n = 1\n",
        "wn = np.sqrt((1/(p*Area))*((((n**4)*(np.pi**4)*E*I)/(L**4))+kf))\n",
        "# First eigenvector - value in the middle of the beam : x=L/2\n",
        "Pn = ((wn**2*p*Area-kf)/(E*I))**(1/4)\n",
        "X1L2 = np.sin(Pn*(L/2))\n",
        "\n",
        "# B. STEP: Calculation of the generalized parameters qi(t)\n",
        "# (dqi2/dt2)+((wn2*Mn+kf*Nn)/Mn)*qi=(Pe/Mn)*sin(Pn*c*t)\n",
        "wn2 = wn**2\n",
        "Nn = (L/2)-(np.sin(2*Pn*L)/(4*Pn))\n",
        "Mn = p*Area*Nn\n",
        "k = Pe/Mn\n",
        "beta = (wn2*Mn+kf*Nn)/Mn\n",
        "\n",
        "# Define the initial state of the system\n",
        "qi_init = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "# Define the function that describes the dynamics of the system\n",
        "def damped_vibration(t, qi):\n",
        "    q1, q2 = qi\n",
        "    dq1dt = q2\n",
        "    dq2dt = k*torch.sin(Pn*c*t) - beta*q1\n",
        "    return torch.tensor([dq1dt, dq2dt])\n",
        "\n",
        "# Define the time steps for the simulation\n",
        "t = torch.linspace(0, 8.0, 801)\n",
        "\n",
        "# Solve the differential equation using automatic differentiation\n",
        "q = odeint(damped_vibration, qi_init, t)\n",
        "q1 = q[:, 0]\n",
        "\n",
        "# Train-test split the data\n",
        "A1mat = np.arange(0, 8.01, 0.01)\n",
        "A2mat = X1L2*q1*100\n",
        "X_train, X_test, y_train, y_test = train_test_split(A1mat.reshape(-1, 1), A2mat.reshape(-1), test_size=0.15, random_state=42)\n",
        "\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# neural network model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the neural network model and define the loss function\n",
        "model = Net()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1000):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    mse_test = mean_squared_error(y_test.numpy(), y_pred.numpy())\n",
        "    mae = mean_absolute_error(y_test.numpy(), y_pred.numpy())\n",
        "    rmse = np.sqrt(mse_test)\n",
        "    r2 = r2_score(y_test.numpy(), y_pred.numpy())\n",
        "\n",
        "# Print the results\n",
        "print('\\nNeural Network Performance Metrics:')\n",
        "print('Test Set Mean Squared Error:', mse_test)\n",
        "print('Mean Absolute Error:', mae)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "print('Coefficient of Determination (R^2):', r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwXCMgGXSxw9",
        "outputId": "a5823e02-baea-41bf-eb81-b61c5e625654"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0fc5fc4418a1>:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
            "<ipython-input-3-0fc5fc4418a1>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test = torch.tensor(y_test, dtype=torch.float32)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([680])) that is different to the input size (torch.Size([680, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.4976\n",
            "Epoch 100, Loss: 0.2600\n",
            "Epoch 200, Loss: 0.2600\n",
            "Epoch 300, Loss: 0.2600\n",
            "Epoch 400, Loss: 0.2600\n",
            "Epoch 500, Loss: 0.2600\n",
            "Epoch 600, Loss: 0.2600\n",
            "Epoch 700, Loss: 0.2600\n",
            "Epoch 800, Loss: 0.2600\n",
            "Epoch 900, Loss: 0.2600\n",
            "\n",
            "Neural Network Performance Metrics:\n",
            "Test Set Mean Squared Error: 0.2420308\n",
            "Mean Absolute Error: 0.40089253\n",
            "Root Mean Squared Error: 0.49196625\n",
            "Coefficient of Determination (R^2): -0.00013233900517706232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import torchdiffeq\n",
        "\n",
        "# Define the input parameters\n",
        "E = 29000000.0 # Modulus of elasticity (kN/m2)\n",
        "I = 0.00423 # Moment of inertia of the cross-section (m4)\n",
        "Area = 0.69 # Area of the cross-section (m2)\n",
        "L = 4.0 # Length of beam (m)\n",
        "p = 27395.515 # Mass/volume unit (kg/m3)\n",
        "kf = 1500.0 # Modulus of the subgrafe reaction (kN/m2)\n",
        "Pe = 500.0 # External moving load (kN)\n",
        "c = 1.00 # Velocity of the external load (m/sec)\n",
        "\n",
        "# A. STEP: Solution of eigenvalue problem -> Calculation of eigenfrequencies and\n",
        "# eigenvectors: EI*(d4X/dx4)+(kf-wn2*p*Area)*X=0\n",
        "# First eigenfrequency\n",
        "n = 1\n",
        "wn = np.sqrt((1/(p*Area))*((((n**4)*(np.pi**4)*E*I)/(L**4))+kf))\n",
        "# First eigenvector - value in the middle of the beam : x=L/2\n",
        "Pn = ((wn**2*p*Area-kf)/(E*I))**(1/4)\n",
        "X1L2 = np.sin(Pn*(L/2))\n",
        "\n",
        "# B. STEP: Calculation of the generalized parameters qi(t)\n",
        "# (dqi2/dt2)+((wn2*Mn+kf*Nn)/Mn)*qi=(Pe/Mn)*sin(Pn*c*t)\n",
        "wn2 = wn**2\n",
        "Nn = (L/2)-(np.sin(2*Pn*L)/(4*Pn))\n",
        "Mn = p*Area*Nn\n",
        "k = Pe/Mn\n",
        "beta = (wn2*Mn+kf*Nn)/Mn\n",
        "q0 = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "def damped_vibration(t, y):\n",
        "    q1, q2 = y[0], y[1]\n",
        "    dq1dt = q2\n",
        "    dq2dt = k*torch.sin(Pn*c*t) - beta*q1\n",
        "    return torch.stack([dq1dt, dq2dt])\n",
        "\n",
        "# Define the initial state of the system as a PyTorch tensor with the requires_grad flag\n",
        "q0 = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "# Define the time points at which to evaluate the solution\n",
        "t = torch.linspace(0.0, 8.0, 801)\n",
        "\n",
        "# Solve the differential equation using automatic differentiation\n",
        "q = torchdiffeq.odeint(damped_vibration, q0, t)\n",
        "\n",
        "# Extract the first component of the solution as the predicted output\n",
        "q1 = q[:, 0]\n",
        "\n",
        "# Train-test split the data\n",
        "A1mat = np.arange(0, 8.01, 0.01)\n",
        "A2mat = X1L2*q1.detach().numpy()*100\n",
        "X_train, X_test, y_train, y_test = train_test_split(A1mat.reshape(-1, 1), A2mat.reshape(-1), test_size=0.15, random_state=42)\n",
        "\n",
        "# Define the neural network model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1, 10)\n",
        "        self.fc2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the neural network model and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the neural network\n",
        "for epoch in range(1000):\n",
        "    # Convert the training data to PyTorch tensors\n",
        "    inputs = torch.from_numpy(X_train).float()\n",
        "    labels = torch.from_numpy(y_train).float()\n",
        "\n",
        "    # Reset the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass through the neural network\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Compute the loss function\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', epoch, 'Loss:', loss.item())\n",
        "\n",
        "# Make predictions on the test set using the trained neural network\n",
        "inputs = torch.from_numpy(X_test).float()\n",
        "y_pred = net(inputs).detach().numpy()\n",
        "\n",
        "# Calculate mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination (R^2)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print('\\nNeural Network Predictions:')\n",
        "results_df = pd.DataFrame({'Time (sec)': X_test.reshape(-1),\n",
        "                           'Displacement (cm) - Actual': y_test,\n",
        "                           'Displacement (cm) - Predicted': y_pred.reshape(-1)})\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print('\\nNeural Network Performance Metrics:')\n",
        "print('Mean Absolute Error:', mae)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "print('Coefficient of Determination (R^2):', r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ02itFIXrEu",
        "outputId": "a2a8e169-dbc4-4608-98f0-47c68bf2ae61"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 3.177027702331543\n",
            "Epoch: 100 Loss: 0.2689160704612732\n",
            "Epoch: 200 Loss: 0.2641291916370392\n",
            "Epoch: 300 Loss: 0.26273518800735474\n",
            "Epoch: 400 Loss: 0.261821448802948\n",
            "Epoch: 500 Loss: 0.2612089514732361\n",
            "Epoch: 600 Loss: 0.26079899072647095\n",
            "Epoch: 700 Loss: 0.26051774621009827\n",
            "Epoch: 800 Loss: 0.2603263556957245\n",
            "Epoch: 900 Loss: 0.2601965367794037\n",
            "\n",
            "Neural Network Predictions:\n",
            " Time (sec)  Displacement (cm) - Actual  Displacement (cm) - Predicted\n",
            "       6.97                   -0.171791                      -0.011568\n",
            "       6.68                   -0.251845                      -0.009747\n",
            "       0.63                    0.040572                      -0.009258\n",
            "       5.34                   -0.792576                      -0.001334\n",
            "       0.66                    0.046353                      -0.009475\n",
            "       6.22                   -0.452861                      -0.006859\n",
            "       3.46                    0.469504                       0.010469\n",
            "       4.90                   -0.744416                       0.001429\n",
            "       5.57                   -0.743134                      -0.002778\n",
            "       4.56                   -0.565237                       0.003563\n",
            "       0.65                    0.044373                      -0.009403\n",
            "       2.86                    0.830905                       0.014236\n",
            "       6.36                   -0.384841                      -0.007738\n",
            "       0.67                    0.048386                      -0.009548\n",
            "       3.27                    0.621702                       0.011662\n",
            "       3.95                   -0.012379                       0.007393\n",
            "       2.44                    0.850352                       0.016873\n",
            "       3.77                    0.172558                       0.008523\n",
            "       6.11                   -0.507989                      -0.006169\n",
            "       5.27                   -0.798334                      -0.000895\n",
            "       6.59                   -0.285093                      -0.009182\n",
            "       5.30                   -0.796443                      -0.001083\n",
            "       6.23                   -0.447894                      -0.006922\n",
            "       7.22                   -0.136267                      -0.013138\n",
            "       3.60                    0.340994                       0.009590\n",
            "       0.30                    0.004606                      -0.006862\n",
            "       2.60                    0.865918                       0.015869\n",
            "       6.38                   -0.375474                      -0.007864\n",
            "       6.01                   -0.557959                      -0.005540\n",
            "       5.71                   -0.693989                      -0.003657\n",
            "       2.15                    0.760441                       0.018694\n",
            "       0.78                    0.074361                      -0.009124\n",
            "       5.79                   -0.660978                      -0.004160\n",
            "       3.83                    0.111210                       0.008146\n",
            "       0.39                    0.010018                      -0.007515\n",
            "       0.23                    0.002088                      -0.006353\n",
            "       6.75                   -0.228633                      -0.010186\n",
            "       3.98                   -0.043199                       0.007204\n",
            "       7.97                   -0.125888                      -0.017846\n",
            "       6.05                   -0.538072                      -0.005792\n",
            "       1.39                    0.336348                       0.018384\n",
            "       2.50                    0.859322                       0.016496\n",
            "       1.74                    0.543466                       0.021268\n",
            "       3.23                    0.649784                       0.011913\n",
            "       5.96                   -0.582484                      -0.005227\n",
            "       5.26                   -0.798767                      -0.000832\n",
            "       4.23                   -0.291171                       0.005635\n",
            "       5.97                   -0.577615                      -0.005289\n",
            "       7.57                   -0.125382                      -0.015335\n",
            "       5.35                   -0.791373                      -0.001397\n",
            "       2.65                    0.865137                       0.015555\n",
            "       1.09                    0.184194                       0.013824\n",
            "       6.67                   -0.255354                      -0.009684\n",
            "       2.94                    0.804537                       0.013734\n",
            "       3.36                    0.553260                       0.011097\n",
            "       3.68                    0.263014                       0.009088\n",
            "       4.46                   -0.491039                       0.004191\n",
            "       3.33                    0.576850                       0.011285\n",
            "       1.98                    0.679243                       0.019761\n",
            "       1.68                    0.507583                       0.021645\n",
            "       4.81                   -0.708798                       0.001993\n",
            "       7.24                   -0.134619                      -0.013263\n",
            "       6.56                   -0.296975                      -0.008994\n",
            "       3.06                    0.751423                       0.012980\n",
            "       2.31                    0.818946                       0.017689\n",
            "       4.22                   -0.281727                       0.005698\n",
            "       0.49                    0.019586                      -0.008241\n",
            "       7.88                   -0.127659                      -0.017281\n",
            "       3.57                    0.369475                       0.009779\n",
            "       0.86                    0.097523                       0.001729\n",
            "       6.77                   -0.222445                      -0.010312\n",
            "       1.92                    0.647056                       0.020138\n",
            "       7.96                   -0.126192                      -0.017783\n",
            "       0.33                    0.006112                      -0.007079\n",
            "       0.31                    0.005077                      -0.006934\n",
            "       1.99                    0.684454                       0.019699\n",
            "       3.65                    0.292572                       0.009276\n",
            "       4.86                   -0.729675                       0.001679\n",
            "       5.95                   -0.587332                      -0.005164\n",
            "       5.69                   -0.701732                      -0.003531\n",
            "       4.28                   -0.337570                       0.005321\n",
            "       1.37                    0.325130                       0.017769\n",
            "       0.72                    0.059363                      -0.009911\n",
            "       0.77                    0.071721                      -0.010274\n",
            "       5.12                   -0.794127                       0.000047\n",
            "       5.51                   -0.760157                      -0.002402\n",
            "       7.87                   -0.127737                      -0.017219\n",
            "       2.75                    0.855138                       0.014927\n",
            "       3.61                    0.331401                       0.009528\n",
            "       0.96                    0.131599                       0.015295\n",
            "       6.93                   -0.180282                      -0.011317\n",
            "       7.16                   -0.142195                      -0.012761\n",
            "       2.81                    0.843649                       0.014550\n",
            "       3.93                    0.008213                       0.007518\n",
            "       6.29                   -0.418389                      -0.007299\n",
            "       6.09                   -0.518037                      -0.006043\n",
            "       7.06                   -0.155618                      -0.012133\n",
            "       7.55                   -0.125266                      -0.015209\n",
            "       0.54                    0.025995                      -0.008604\n",
            "       7.51                   -0.125197                      -0.014958\n",
            "       1.55                    0.429553                       0.022461\n",
            "       4.32                   -0.373621                       0.005070\n",
            "       0.97                    0.135316                       0.016652\n",
            "       1.20                    0.235504                       0.012540\n",
            "       0.76                    0.069138                      -0.010202\n",
            "       2.04                    0.709766                       0.019385\n",
            "       3.52                    0.415858                       0.010093\n",
            "       7.65                   -0.126197                      -0.015837\n",
            "       2.08                    0.729046                       0.019133\n",
            "       3.67                    0.272905                       0.009151\n",
            "       4.25                   -0.309899                       0.005509\n",
            "       5.16                   -0.797533                      -0.000204\n",
            "       0.84                    0.091391                      -0.000984\n",
            "       0.10                    0.000173                      -0.005409\n",
            "       7.21                   -0.137151                      -0.013075\n",
            "       1.10                    0.188612                       0.011954\n",
            "       5.22                   -0.799500                      -0.000581\n",
            "       7.78                   -0.127658                      -0.016654\n",
            "       2.09                    0.733720                       0.019071\n",
            "       2.35                    0.830274                       0.017438\n",
            "       3.16                    0.695215                       0.012353\n",
            "\n",
            "Neural Network Performance Metrics:\n",
            "Mean Absolute Error: 0.39386946\n",
            "Root Mean Squared Error: 0.48467174\n",
            "Coefficient of Determination (R^2): 0.02930636507629447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchdiffeq\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Define the parameters of the system\n",
        "m = 1\n",
        "k = 1\n",
        "beta = 0.1\n",
        "\n",
        "# Define the initial conditions of the system\n",
        "q0 = [1, 0]\n",
        "\n",
        "# Define the forcing function\n",
        "Pn = 1\n",
        "c = 1\n",
        "\n",
        "# Define the differential equation for the damped vibration system\n",
        "def damped_vibration(t, y):\n",
        "    q1, q2 = y[0], y[1] if len(y) > 1 else 0\n",
        "    dq1dt = q2\n",
        "    dq2dt = k*torch.sin(Pn*c*t) - beta*q1\n",
        "    return torch.stack([dq1dt, dq2dt])\n",
        "\n",
        "# Define the initial state of the system as a PyTorch tensor with the requires_grad flag\n",
        "q0 = torch.tensor([1., 0.], requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "# Define the time points for which to solve the differential equation\n",
        "t = torch.linspace(0, 20, 1000)\n",
        "\n",
        "# Solve the differential equation using automatic differentiation\n",
        "q = torchdiffeq.odeint(damped_vibration, q0, t)\n",
        "\n",
        "# Extract the first component of the solution as the predicted output\n",
        "y_pred = q[:, 0]\n",
        "\n",
        "# Concatenate the interpolated data with the original\n",
        "t_all = torch.cat((t, t_new))\n",
        "y_all = torch.cat((y_pred, y_new))\n",
        "\n",
        "# Convert the data to NumPy arrays\n",
        "t_all = t_all.detach().numpy()\n",
        "y_all = y_all.detach().numpy()\n",
        "\n",
        "# Define the neural network model\n",
        "class Net1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1, 10)\n",
        "        self.fc2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Train the neural network\n",
        "X_train, X_test, y_train, y_test = train_test_split(t_all, y_all, test_size=0.15, random_state=42)\n",
        "X_train = X_train.reshape(-1, 1)\n",
        "X_test = X_test.reshape(-1, 1)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "net = Net1()\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    inputs = torch.autograd.Variable(torch.Tensor(X_train))\n",
        "    targets = torch.autograd.Variable(torch.Tensor(y_train))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = net(inputs)\n",
        "    loss = criterion(out, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n",
        "\n",
        "\n",
        "# Test the neural network\n",
        "X_test = torch.tensor(X_test, requires_grad=True, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, requires_grad=True, dtype=torch.float32)\n",
        "y_pred = net(X_test)\n",
        "r2 = r2_score(y_test.detach().numpy(), y_pred.detach().numpy())\n",
        "print(f\"Coefficient of Determination (R^2): {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "3HjbwiageAF8",
        "outputId": "644281d7-80f2-4b60-8e60-796bd8a44529"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ee0cc55d8ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Concatenate the interpolated data with the original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mt_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0my_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 't_new' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import torchdiffeq\n",
        "\n",
        "# Define the input parameters\n",
        "E = 29000000.0 # Modulus of elasticity (kN/m2)\n",
        "I = 0.00423 # Moment of inertia of the cross-section (m4)\n",
        "Area = 0.69 # Area of the cross-section (m2)\n",
        "L = 4.0 # Length of beam (m)\n",
        "p = 27395.515 # Mass/volume unit (kg/m3)\n",
        "kf = 1500.0 # Modulus of the subgrafe reaction (kN/m2)\n",
        "Pe = 500.0 # External moving load (kN)\n",
        "c = 1.00 # Velocity of the external load (m/sec)\n",
        "\n",
        "# A. STEP: Solution of eigenvalue problem -> Calculation of eigenfrequencies and\n",
        "# eigenvectors: EI*(d4X/dx4)+(kf-wn2*p*Area)*X=0\n",
        "# First eigenfrequency\n",
        "n = 1\n",
        "wn = np.sqrt((1/(p*Area))*((((n**4)*(np.pi**4)*E*I)/(L**4))+kf))\n",
        "# First eigenvector - value in the middle of the beam : x=L/2\n",
        "Pn = ((wn**2*p*Area-kf)/(E*I))**(1/4)\n",
        "X1L2 = np.sin(Pn*(L/2))\n",
        "\n",
        "# B. STEP: Calculation of the generalized parameters qi(t)\n",
        "# (dqi2/dt2)+((wn2*Mn+kf*Nn)/Mn)*qi=(Pe/Mn)*sin(Pn*c*t)\n",
        "wn2 = wn**2\n",
        "Nn = (L/2)-(np.sin(2*Pn*L)/(4*Pn))\n",
        "Mn = p*Area*Nn\n",
        "k = Pe/Mn\n",
        "beta = (wn2*Mn+kf*Nn)/Mn\n",
        "q0 = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "def damped_vibration(t, y):\n",
        "    q1, q2 = y[0], y[1]\n",
        "    dq1dt = q2\n",
        "    dq2dt = k*torch.sin(Pn*c*t) - beta*q1\n",
        "    return torch.stack([dq1dt, dq2dt])\n",
        "\n",
        "# Define the initial state of the system as a PyTorch tensor with the requires_grad flag\n",
        "q0 = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "# Define the time points at which to evaluate the solution\n",
        "t = torch.linspace(0.0, 16.0, 1601)  # Change the range and step size to increase the dataset size\n",
        "\n",
        "\n",
        "# Solve the differential equation using automatic differentiation\n",
        "q = torchdiffeq.odeint(damped_vibration, q0, t)\n",
        "\n",
        "# Extract the first component of the solution as the predicted output\n",
        "q1 = q[:, 0]\n",
        "\n",
        "A1mat = np.linspace(0, 16.0, 1601)\n",
        "A2mat = X1L2*q1.detach().numpy()*100\n",
        "X_train, X_test, y_train, y_test = train_test_split(A1mat.reshape(-1, 1), A2mat.reshape(-1), test_size=0.15, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the neural network model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1, 10)\n",
        "        self.fc2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the neural network model and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the neural network\n",
        "for epoch in range(1000):\n",
        "    # Convert the training data to PyTorch tensors\n",
        "    inputs = torch.from_numpy(X_train).float()\n",
        "    labels = torch.from_numpy(y_train).float()\n",
        "\n",
        "    # Reset the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass through the neural network\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Compute the loss function\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', epoch, 'Loss:', loss.item())\n",
        "\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'num_hidden_layers': [1, 2, 3],\n",
        "    'hidden_layer_size': [10, 20, 50],\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'num_epochs': [500, 1000, 2000],\n",
        "}\n",
        "\n",
        "# Define the neural network model with variable hidden layers and hidden layer size\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_hidden_layers, hidden_layer_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_layer = torch.nn.Linear(1, hidden_layer_size)\n",
        "        self.hidden_layers = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers - 1)\n",
        "        ])\n",
        "        self.output_layer = torch.nn.Linear(hidden_layer_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.input_layer(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = torch.nn.functional.relu(hidden_layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "best_net = None\n",
        "best_loss = float('inf')\n",
        "best_params = None\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    net = Net(params['num_hidden_layers'], params['hidden_layer_size'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "    # Train the neural network\n",
        "    for epoch in range(params['num_epochs']):\n",
        "        inputs = torch.from_numpy(X_train).float()\n",
        "        labels = torch.from_numpy(y_train).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Make predictions on the test set using the trained neural network\n",
        "    inputs = torch.from_numpy(X_test).float()\n",
        "    y_pred = net(inputs).detach().numpy()\n",
        "\n",
        "    # Calculate mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination (R^2)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Save the best model and parameters\n",
        "    if mae < best_loss:\n",
        "        best_net = net\n",
        "        best_loss = mae\n",
        "        best_params = params\n",
        "\n",
        "# Print the best parameters\n",
        "print('Best Hyperparameters:')\n",
        "print(best_params)\n",
        "\n",
        "\n",
        "# Make predictions on the test set using the trained neural network\n",
        "inputs = torch.from_numpy(X_test).float()\n",
        "y_pred = net(inputs).detach().numpy()\n",
        "\n",
        "# Calculate mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination (R^2)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print('\\nNeural Network Predictions:')\n",
        "results_df = pd.DataFrame({'Time (sec)': X_test.reshape(-1),\n",
        "                           'Displacement (cm) - Actual': y_test,\n",
        "                           'Displacement (cm) - Predicted': y_pred.reshape(-1)})\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print('\\nNeural Network Performance Metrics:')\n",
        "print('Mean Absolute Error:', mae)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "print('Coefficient of Determination (R^2):', r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrzu1E-xe1pY",
        "outputId": "3651a308-150d-4326-c985-e868752c42e7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 17.146499633789062\n",
            "Epoch: 100 Loss: 2.000664472579956\n",
            "Epoch: 200 Loss: 0.2972821295261383\n",
            "Epoch: 300 Loss: 0.27331313490867615\n",
            "Epoch: 400 Loss: 0.26331183314323425\n",
            "Epoch: 500 Loss: 0.25550520420074463\n",
            "Epoch: 600 Loss: 0.25498852133750916\n",
            "Epoch: 700 Loss: 0.25491979718208313\n",
            "Epoch: 800 Loss: 0.25489741563796997\n",
            "Epoch: 900 Loss: 0.2548879086971283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1360])) that is different to the input size (torch.Size([1360, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'hidden_layer_size': 50, 'learning_rate': 0.1, 'num_epochs': 1000, 'num_hidden_layers': 2}\n",
            "\n",
            "Neural Network Predictions:\n",
            " Time (sec)  Displacement (cm) - Actual  Displacement (cm) - Predicted\n",
            "       5.26                   -0.798767                      -0.020875\n",
            "       3.54                    0.397477                      -0.022597\n",
            "       1.68                    0.507583                      -0.022946\n",
            "       1.35                    0.314036                      -0.022866\n",
            "       9.37                    0.420930                      -0.019079\n",
            "      15.45                   -0.247737                      -0.022326\n",
            "      15.96                   -0.235507                      -0.022900\n",
            "       2.37                    0.835396                      -0.023127\n",
            "       4.78                   -0.694977                      -0.021356\n",
            "       6.50                   -0.321845                      -0.020632\n",
            "       5.14                   -0.796042                      -0.020995\n",
            "      15.87                   -0.245393                      -0.022798\n",
            "       9.68                    0.631962                      -0.018897\n",
            "      15.70                   -0.253316                      -0.022607\n",
            "       5.84                   -0.638901                      -0.020295\n",
            "       0.65                    0.044373                      -0.022700\n",
            "       0.30                    0.004606                      -0.023336\n",
            "       9.04                    0.202515                      -0.019388\n",
            "       3.67                    0.272905                      -0.022467\n",
            "      13.19                   -0.711167                      -0.019785\n",
            "      13.87                   -0.503243                      -0.020549\n",
            "       4.85                   -0.725717                      -0.021286\n",
            "      15.49                   -0.249468                      -0.022371\n",
            "       3.74                    0.202963                      -0.022397\n",
            "       0.29                    0.004165                      -0.023450\n",
            "      13.25                   -0.702742                      -0.019852\n",
            "       9.58                    0.565816                      -0.018947\n",
            "      14.67                   -0.257701                      -0.021449\n",
            "       3.32                    0.584545                      -0.022817\n",
            "       7.12                   -0.147013                      -0.021025\n",
            "       0.70                    0.054808                      -0.022794\n",
            "       2.89                    0.821877                      -0.023247\n",
            "       9.87                    0.746009                      -0.018801\n",
            "      15.27                   -0.238993                      -0.022124\n",
            "       4.16                   -0.224038                      -0.021976\n",
            "      10.88                    0.805007                      -0.018290\n",
            "       3.24                    0.642905                      -0.022897\n",
            "       6.17                   -0.477843                      -0.020423\n",
            "       3.80                    0.141959                      -0.022337\n",
            "       1.73                    0.537511                      -0.022959\n",
            "       6.34                   -0.394312                      -0.020531\n",
            "       7.01                   -0.164108                      -0.020956\n",
            "      14.14                   -0.398003                      -0.020853\n",
            "       0.23                    0.002088                      -0.023605\n",
            "      15.24                   -0.237657                      -0.022090\n",
            "       0.32                    0.005579                      -0.023100\n",
            "       5.66                   -0.712922                      -0.020475\n",
            "       0.99                    0.142919                      -0.022779\n",
            "       5.18                   -0.798604                      -0.020955\n",
            "      11.88                   -0.064868                      -0.018312\n",
            "      13.53                   -0.631659                      -0.020167\n",
            "      11.76                    0.055365                      -0.018177\n",
            "       8.07                   -0.121049                      -0.020294\n",
            "       3.94                   -0.002086                      -0.022197\n",
            "      11.05                    0.702991                      -0.018205\n",
            "      14.53                   -0.282865                      -0.021292\n",
            "       3.52                    0.415858                      -0.022617\n",
            "       5.70                   -0.697888                      -0.020435\n",
            "       4.15                   -0.214267                      -0.021986\n",
            "       6.11                   -0.507989                      -0.020385\n",
            "       3.00                    0.779986                      -0.023137\n",
            "       0.69                    0.052613                      -0.022797\n",
            "      10.65                    0.892337                      -0.018407\n",
            "      10.36                    0.913494                      -0.018553\n",
            "       7.82                   -0.127846                      -0.020528\n",
            "       0.59                    0.033592                      -0.022541\n",
            "       6.36                   -0.384842                      -0.020544\n",
            "      10.18                    0.878466                      -0.018644\n",
            "       7.59                   -0.125540                      -0.020743\n",
            "       6.77                   -0.222445                      -0.020804\n",
            "      12.93                   -0.711930                      -0.019493\n",
            "       5.27                   -0.798334                      -0.020865\n",
            "      15.54                   -0.251309                      -0.022427\n",
            "       8.17                   -0.112310                      -0.020201\n",
            "       2.44                    0.850352                      -0.023265\n",
            "      10.43                    0.917435                      -0.018518\n",
            "      15.25                   -0.238092                      -0.022101\n",
            "      10.74                    0.865474                      -0.018361\n",
            "      14.63                   -0.263950                      -0.021404\n",
            "      14.42                   -0.309030                      -0.021168\n",
            "      12.06                   -0.236463                      -0.018514\n",
            "      11.08                    0.682007                      -0.018189\n",
            "      12.92                   -0.710691                      -0.019481\n",
            "       5.35                   -0.791373                      -0.020785\n",
            "      15.68                   -0.253480                      -0.022585\n",
            "      11.59                    0.227527                      -0.017986\n",
            "       2.98                    0.788619                      -0.023157\n",
            "       7.64                   -0.126075                      -0.020696\n",
            "       3.44                    0.486849                      -0.022697\n",
            "       5.30                   -0.796443                      -0.020835\n",
            "      10.85                    0.819828                      -0.018306\n",
            "       5.34                   -0.792576                      -0.020795\n",
            "      12.86                   -0.701157                      -0.019414\n",
            "       4.71                   -0.659018                      -0.021426\n",
            "       6.10                   -0.513015                      -0.020379\n",
            "       1.85                    0.607793                      -0.022988\n",
            "       7.07                   -0.154066                      -0.020994\n",
            "       0.49                    0.019586                      -0.022274\n",
            "      12.21                   -0.366036                      -0.018683\n",
            "       8.18                   -0.111190                      -0.020192\n",
            "       1.24                    0.255558                      -0.022840\n",
            "       9.03                    0.196445                      -0.019397\n",
            "       3.39                    0.528935                      -0.022747\n",
            "       8.34                   -0.086303                      -0.020042\n",
            "       9.07                    0.220979                      -0.019360\n",
            "       3.09                    0.735671                      -0.023047\n",
            "       3.66                    0.282758                      -0.022477\n",
            "       3.61                    0.331401                      -0.022527\n",
            "       6.01                   -0.557959                      -0.020322\n",
            "      11.78                    0.035148                      -0.018199\n",
            "      14.06                   -0.427982                      -0.020763\n",
            "       4.05                   -0.114566                      -0.022086\n",
            "      11.17                    0.614220                      -0.018144\n",
            "      13.28                   -0.697506                      -0.019886\n",
            "       5.61                   -0.730361                      -0.020525\n",
            "      12.59                   -0.612525                      -0.019110\n",
            "       2.47                    0.855294                      -0.023324\n",
            "       1.09                    0.184194                      -0.022803\n",
            "       2.20                    0.780838                      -0.023073\n",
            "       2.03                    0.704807                      -0.023031\n",
            "       8.47                   -0.055377                      -0.019921\n",
            "      12.26                   -0.405701                      -0.018739\n",
            "       2.75                    0.855138                      -0.023288\n",
            "       4.62                   -0.605392                      -0.021516\n",
            "      13.65                   -0.589434                      -0.020302\n",
            "       3.50                    0.433998                      -0.022637\n",
            "       2.70                    0.861558                      -0.023282\n",
            "       5.98                   -0.572728                      -0.020303\n",
            "       0.51                    0.022012                      -0.022328\n",
            "       0.76                    0.069138                      -0.022772\n",
            "       5.51                   -0.760157                      -0.020625\n",
            "      14.90                   -0.235761                      -0.021708\n",
            "      14.61                   -0.267357                      -0.021381\n",
            "      11.14                    0.637589                      -0.018159\n",
            "       6.14                   -0.492907                      -0.020404\n",
            "       1.62                    0.471508                      -0.022932\n",
            "       1.23                    0.250480                      -0.022837\n",
            "      14.10                   -0.412806                      -0.020808\n",
            "       4.38                   -0.425713                      -0.021756\n",
            "       8.55                   -0.031165                      -0.019846\n",
            "      12.45                   -0.537263                      -0.018953\n",
            "       2.51                    0.860459                      -0.023349\n",
            "       2.71                    0.860502                      -0.023283\n",
            "       1.84                    0.602060                      -0.022985\n",
            "       0.67                    0.048386                      -0.022754\n",
            "       4.83                   -0.717474                      -0.021306\n",
            "       1.41                    0.347682                      -0.022881\n",
            "      15.34                   -0.242377                      -0.022202\n",
            "       9.32                    0.386332                      -0.019126\n",
            "       5.28                   -0.797802                      -0.020855\n",
            "       4.64                   -0.618008                      -0.021496\n",
            "       7.15                   -0.143333                      -0.021044\n",
            "      10.01                    0.815707                      -0.018730\n",
            "       3.06                    0.751423                      -0.023077\n",
            "       7.65                   -0.126197                      -0.020687\n",
            "      11.37                    0.441485                      -0.018043\n",
            "      12.55                   -0.593012                      -0.019065\n",
            "       5.91                   -0.606487                      -0.020258\n",
            "      11.28                    0.522547                      -0.018088\n",
            "       2.97                    0.792768                      -0.023167\n",
            "      13.80                   -0.531313                      -0.020471\n",
            "       9.38                    0.427872                      -0.019070\n",
            "      12.42                   -0.518653                      -0.018919\n",
            "       7.30                   -0.130589                      -0.021014\n",
            "      11.46                    0.356120                      -0.017997\n",
            "      12.08                   -0.254562                      -0.018537\n",
            "       8.88                    0.111141                      -0.019537\n",
            "      13.70                   -0.570542                      -0.020358\n",
            "       5.60                   -0.733656                      -0.020535\n",
            "       9.25                    0.338448                      -0.019192\n",
            "       3.82                    0.121474                      -0.022317\n",
            "      11.21                    0.581930                      -0.018124\n",
            "       2.39                    0.840147                      -0.023166\n",
            "      12.99                   -0.717306                      -0.019560\n",
            "       4.51                   -0.529230                      -0.021626\n",
            "       3.16                    0.695215                      -0.022977\n",
            "      11.64                    0.177064                      -0.018042\n",
            "       3.31                    0.592153                      -0.022827\n",
            "       9.78                    0.694311                      -0.018846\n",
            "       5.54                   -0.751978                      -0.020595\n",
            "       5.90                   -0.611211                      -0.020252\n",
            "       3.03                    0.766201                      -0.023107\n",
            "      13.11                   -0.717861                      -0.019695\n",
            "       1.70                    0.519579                      -0.022951\n",
            "      10.89                    0.799846                      -0.018285\n",
            "       2.61                    0.865983                      -0.023270\n",
            "       5.75                   -0.677871                      -0.020385\n",
            "       0.44                    0.014290                      -0.022142\n",
            "       4.68                   -0.642049                      -0.021456\n",
            "       2.59                    0.865743                      -0.023267\n",
            "       5.44                   -0.776490                      -0.020695\n",
            "       9.41                    0.448719                      -0.019042\n",
            "       3.53                    0.406697                      -0.022607\n",
            "       3.98                   -0.043199                      -0.022156\n",
            "       8.42                   -0.068480                      -0.019967\n",
            "       7.52                   -0.125192                      -0.020809\n",
            "       5.29                   -0.797172                      -0.020845\n",
            "      10.37                    0.914400                      -0.018548\n",
            "       1.15                    0.211455                      -0.022818\n",
            "       7.36                   -0.127810                      -0.020958\n",
            "       8.36                   -0.082202                      -0.020023\n",
            "       8.67                    0.012770                      -0.019734\n",
            "       1.75                    0.549408                      -0.022963\n",
            "       3.42                    0.503907                      -0.022717\n",
            "      13.18                   -0.712295                      -0.019774\n",
            "       4.13                   -0.194603                      -0.022006\n",
            "      12.12                   -0.290046                      -0.018582\n",
            "       0.78                    0.074361                      -0.022765\n",
            "      11.45                    0.365783                      -0.018002\n",
            "      14.75                   -0.247429                      -0.021539\n",
            "      10.54                    0.912081                      -0.018462\n",
            "      13.91                   -0.487162                      -0.020594\n",
            "       6.15                   -0.487882                      -0.020411\n",
            "       5.85                   -0.634370                      -0.020285\n",
            "       1.98                    0.679243                      -0.023019\n",
            "       0.15                    0.000582                      -0.023794\n",
            "       7.86                   -0.127795                      -0.020491\n",
            "      11.33                    0.478110                      -0.018063\n",
            "       2.74                    0.856651                      -0.023287\n",
            "      12.25                   -0.397924                      -0.018728\n",
            "      10.31                    0.907278                      -0.018578\n",
            "       7.20                   -0.138076                      -0.021076\n",
            "       8.93                    0.138311                      -0.019491\n",
            "      13.23                   -0.705859                      -0.019830\n",
            "       6.72                   -0.238287                      -0.020772\n",
            "      12.46                   -0.543277                      -0.018964\n",
            "       4.93                   -0.754320                      -0.021206\n",
            "       6.54                   -0.305105                      -0.020658\n",
            "       1.99                    0.684454                      -0.023022\n",
            "       6.07                   -0.528068                      -0.020360\n",
            "      13.30                   -0.693657                      -0.019909\n",
            "       9.06                    0.214783                      -0.019369\n",
            "      12.27                   -0.413399                      -0.018750\n",
            "      15.92                   -0.240447                      -0.022855\n",
            "       3.51                    0.424959                      -0.022627\n",
            "      11.85                   -0.035101                      -0.018278\n",
            "       3.81                    0.131724                      -0.022327\n",
            "       8.19                   -0.110022                      -0.020182\n",
            "       9.83                    0.723619                      -0.018821\n",
            "      11.51                    0.307250                      -0.017972\n",
            "      10.04                    0.828662                      -0.018715\n",
            "\n",
            "Neural Network Performance Metrics:\n",
            "Mean Absolute Error: 0.45091975\n",
            "Root Mean Squared Error: 0.5288168\n",
            "Coefficient of Determination (R^2): -0.0035754898375668365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autokeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGsq0d2ujlDX",
        "outputId": "3fc50195-b032-4b92-aedc-348c2a514cd6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autokeras\n",
            "  Downloading autokeras-1.1.0-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.6/148.6 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-tuner>=1.1.0\n",
            "  Downloading keras_tuner-1.3.0-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from autokeras) (23.0)\n",
            "Collecting keras-nlp>=0.4.0\n",
            "  Downloading keras_nlp-0.4.1-py3-none-any.whl (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.8/466.8 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from autokeras) (1.4.4)\n",
            "Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from autokeras) (2.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from keras-nlp>=0.4.0->autokeras) (1.22.4)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from keras-nlp>=0.4.0->autokeras) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner>=1.1.0->autokeras) (2.27.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from keras-tuner>=1.1.0->autokeras) (7.9.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (23.3.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (0.31.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (15.0.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (63.4.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (4.5.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (3.8.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (1.51.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (3.19.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.8.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->autokeras) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->autokeras) (2022.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->autokeras) (0.40.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (2.0.12)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text->keras-nlp>=0.4.0->autokeras) (0.13.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->keras-tuner>=1.1.0->autokeras) (0.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (6.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner>=1.1.0->autokeras) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (2.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->keras-tuner>=1.1.0->autokeras) (0.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (3.2.2)\n",
            "Installing collected packages: kt-legacy, jedi, tensorflow-text, keras-tuner, keras-nlp, autokeras\n",
            "Successfully installed autokeras-1.1.0 jedi-0.18.2 keras-nlp-0.4.1 keras-tuner-1.3.0 kt-legacy-1.0.4 tensorflow-text-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import autokeras as ak\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the input parameters\n",
        "E = 29000000.0 # Modulus of elasticity (kN/m2)\n",
        "I = 0.00423 # Moment of inertia of the cross-section (m4)\n",
        "Area = 0.69 # Area of the cross-section (m2)\n",
        "L = 4.0 # Length of beam (m)\n",
        "p = 27395.515 # Mass/volume unit (kg/m3)\n",
        "kf = 1500.0 # Modulus of the subgrafe reaction (kN/m2)\n",
        "Pe = 500.0 # External moving load (kN)\n",
        "c = 1.00 # Velocity of the external load (m/sec)\n",
        "\n",
        "# A. STEP: Solution of eigenvalue problem -> Calculation of eigenfrequencies and\n",
        "# eigenvectors: EI*(d4X/dx4)+(kf-wn2*p*Area)*X=0\n",
        "# First eigenfrequency\n",
        "n = 1\n",
        "wn = np.sqrt((1/(p*Area))*((((n**4)*(np.pi**4)*E*I)/(L**4))+kf))\n",
        "# First eigenvector - value in the middle of the beam : x=L/2\n",
        "Pn = ((wn**2*p*Area-kf)/(E*I))**(1/4)\n",
        "X1L2 = np.sin(Pn*(L/2))\n",
        "\n",
        "# B. STEP: Calculation of the generalized parameters qi(t)\n",
        "# (dqi2/dt2)+((wn2*Mn+kf*Nn)/Mn)*qi=(Pe/Mn)*sin(Pn*c*t)\n",
        "wn2 = wn**2\n",
        "Nn = (L/2)-(np.sin(2*Pn*L)/(4*Pn))\n",
        "Mn = p*Area*Nn\n",
        "k = Pe/Mn\n",
        "\n",
        "# Generate the input and output data\n",
        "A1mat = np.linspace(0, 16.0, 1601)\n",
        "A2mat = X1L2*np.sin(Pn*c*A1mat)*100\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(A1mat.reshape(-1, 1), A2mat.reshape(-1), test_size=0.15, random_state=42)\n",
        "\n",
        "# Define the AutoKeras model\n",
        "reg = ak.StructuredDataRegressor(max_trials=10)\n",
        "\n",
        "# Fit the AutoKeras model to the training data\n",
        "reg.fit(X_train, y_train, verbose=0)\n",
        "\n",
        "# Generate predictions using the AutoKeras model\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculate mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination (R^2)\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "rmse = np.sqrt(np.mean(np.square(y_test - y_pred)))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print('\\nAutoKeras Predictions:')\n",
        "results_df = pd.DataFrame({'Time (sec)': X_test.reshape(-1),\n",
        "                           'Displacement (cm) - Actual': y_test,\n",
        "                           'Displacement (cm) - Predicted': y_pred.reshape(-1)})\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print('\\nAutoKeras Performance Metrics:')\n",
        "print('Mean Absolute Error:', mae)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "print('Coefficient of Determination (R^2):', r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM50kwOBxIy2",
        "outputId": "b820607f-a8f2-4efe-9df6-3fd2e654eff9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "\n",
            "AutoKeras Predictions:\n",
            " Time (sec)  Displacement (cm) - Actual  Displacement (cm) - Predicted\n",
            "       5.26                  -83.580736                     -96.674835\n",
            "       3.54                   35.347484                      -7.494800\n",
            "       1.68                   96.858316                      52.445015\n",
            "       1.35                   87.249601                      55.768921\n",
            "       9.37                   88.006330                      86.674728\n",
            "      15.45                  -41.865974                     -86.000092\n",
            "      15.96                   -3.141076                     -97.286972\n",
            "       2.37                   95.807290                      45.231464\n",
            "       4.78                  -57.500525                     -76.688538\n",
            "       6.50                  -92.387953                     -87.077583\n",
            "       5.14                  -78.043041                     -94.219566\n",
            "      15.87                  -10.192446                     -95.295166\n",
            "       9.68                   96.858316                      90.393982\n",
            "      15.70                  -23.344536                     -91.532867\n",
            "       5.84                  -99.211470                     -98.998116\n",
            "       0.65                   48.862124                      62.819645\n",
            "       0.30                   23.344536                      66.345200\n",
            "       9.04                   72.896863                      79.707115\n",
            "       3.67                   25.628937                     -14.783700\n",
            "      13.19                  -80.437564                     -32.771580\n",
            "      13.87                  -99.479214                     -51.028408\n",
            "       4.85                  -61.909395                     -80.468727\n",
            "      15.49                  -38.992769                     -86.885345\n",
            "       3.74                   20.278730                     -18.708479\n",
            "       0.29                   22.580127                      66.446060\n",
            "      13.25                  -83.146961                     -35.023258\n",
            "       9.58                   94.608536                      89.194214\n",
            "      14.67                  -86.471344                     -68.737915\n",
            "       3.32                   50.904142                       4.722133\n",
            "       7.12                  -63.742399                     -49.270103\n",
            "       0.70                   52.249856                      62.316010\n",
            "       2.89                   76.548321                      26.500164\n",
            "       9.87                   99.479214                      92.171478\n",
            "      15.27                  -54.244154                     -82.016518\n",
            "       4.16                  -12.533323                     -42.219917\n",
            "      10.88                   77.051324                      62.089432\n",
            "       3.24                   56.208338                       8.773826\n",
            "       6.17                  -99.109975                     -97.713768\n",
            "       3.80                   15.643447                     -22.072622\n",
            "       1.73                   97.759994                      51.941395\n",
            "       6.34                  -96.455742                     -92.513138\n",
            "       7.01                  -70.153143                     -57.421608\n",
            "      14.14                  -99.396096                     -57.008434\n",
            "       0.23                   17.966075                      67.051300\n",
            "      15.24                  -56.208338                     -81.352585\n",
            "       0.32                   24.868989                      66.143539\n",
            "       5.66                  -96.455742                     -98.357643\n",
            "       0.99                   70.153143                      59.395027\n",
            "       5.18                  -79.968466                     -95.231834\n",
            "      11.88                    9.410831                      21.018969\n",
            "      13.53                  -93.263902                     -43.494514\n",
            "      11.76                   18.738131                      25.946341\n",
            "       8.07                    5.495018                      21.198357\n",
            "       3.94                    4.710645                     -29.909910\n",
            "      11.05                   67.880075                      55.101456\n",
            "      14.53                  -91.460716                     -65.639542\n",
            "       3.52                   36.812455                      -6.373433\n",
            "       5.70                  -97.236992                     -98.499962\n",
            "       4.15                  -11.753740                     -41.660385\n",
            "       6.11                  -99.627038                     -98.697052\n",
            "       3.00                   70.710678                      20.929052\n",
            "       0.69                   51.578590                      62.416733\n",
            "      10.65                   87.249601                      71.543777\n",
            "      10.36                   96.029369                      83.281845\n",
            "       7.82                  -14.090123                       2.654060\n",
            "       0.59                   44.697862                      63.423988\n",
            "       6.36                  -96.029369                     -91.833900\n",
            "      10.18                   99.002366                      89.549515\n",
            "       7.59                  -31.647697                     -14.406732\n",
            "       6.77                  -82.264052                     -75.266319\n",
            "      12.93                  -66.718277                     -22.095617\n",
            "       5.27                  -84.009355                     -96.765640\n",
            "      15.54                  -35.347484                     -87.991913\n",
            "       8.17                   13.312134                      28.616129\n",
            "       2.44                   94.088077                      44.237907\n",
            "      10.43                   94.351216                      80.508400\n",
            "      15.25                  -55.557023                     -81.573898\n",
            "      10.74                   83.580736                      67.844261\n",
            "      14.63                  -88.006330                     -67.852646\n",
            "      14.42                  -94.608536                     -63.205132\n",
            "      12.06                   -4.710645                      13.627866\n",
            "      11.08                   66.131187                      53.868298\n",
            "      12.92                  -66.131187                     -21.684988\n",
            "       5.35                  -87.249601                     -97.256676\n",
            "      15.68                  -24.868989                     -91.090256\n",
            "      11.59                   31.647697                      32.926781\n",
            "       2.98                   71.812630                      21.941998\n",
            "       7.64                  -27.899111                     -10.697883\n",
            "       3.44                   42.577929                      -1.887985\n",
            "       5.30                  -85.264016                     -97.038017\n",
            "      10.85                   78.531693                      63.322601\n",
            "       5.34                  -86.863151                     -97.221161\n",
            "      12.86                  -62.524266                     -19.221275\n",
            "       4.71                  -52.917900                     -72.873055\n",
            "       6.10                  -99.691733                     -98.794609\n",
            "       1.85                   99.306846                      50.732700\n",
            "       7.07                  -66.718277                     -52.976608\n",
            "       0.49                   37.541557                      64.431213\n",
            "      12.21                  -16.418685                       7.468667\n",
            "       8.18                   14.090123                      29.357924\n",
            "       1.24                   82.708057                      56.876896\n",
            "       9.03                   72.356978                      79.317886\n",
            "       3.39                   46.097437                       0.915437\n",
            "       8.34                   26.387305                      41.150055\n",
            "       9.07                   74.489406                      80.820633\n",
            "       3.09                   65.540017                      16.370832\n",
            "       3.66                   26.387305                     -14.223012\n",
            "       3.61                   30.153796                     -11.419572\n",
            "       6.01                  -99.996916                     -99.496429\n",
            "      11.78                   17.192910                      25.125135\n",
            "      14.06                  -99.888987                     -55.237946\n",
            "       4.05                   -3.925982                     -36.064922\n",
            "      11.17                   60.668235                      50.172604\n",
            "      13.28                  -84.432793                     -35.964046\n",
            "       5.61                  -95.345417                     -98.179947\n",
            "      12.59                  -44.697862                      -8.134690\n",
            "       2.47                   93.263902                      43.699379\n",
            "       1.09                   75.528181                      58.387756\n",
            "       2.20                   98.768834                      47.207348\n",
            "       2.03                   99.972243                      48.919666\n",
            "       8.47                   36.081083                      50.536461\n",
            "      12.26                  -20.278730                       5.415601\n",
            "       2.75                   83.146961                      33.590733\n",
            "       4.62                  -46.792981                     -67.901253\n",
            "      13.65                  -96.245524                     -46.153526\n",
            "       3.50                   38.268343                      -5.252061\n",
            "       2.70                   85.264016                      36.123108\n",
            "       5.98                  -99.987663                     -99.490150\n",
            "       0.51                   38.992769                      64.229782\n",
            "       0.76                   56.208338                      61.711651\n",
            "       5.51                  -92.685660                     -97.824837\n",
            "      14.90                  -76.040597                     -73.828011\n",
            "      14.61                  -88.741345                     -67.410011\n",
            "      11.14                   62.524266                      51.404476\n",
            "       6.14                  -99.396096                     -98.286591\n",
            "       1.62                   95.579301                      53.049351\n",
            "       1.23                   82.264052                      56.977623\n",
            "      14.10                  -99.691733                     -56.123188\n",
            "       4.38                  -29.404033                     -54.529949\n",
            "       8.55                   41.865974                      56.163380\n",
            "      12.45                  -34.611706                      -2.386075\n",
            "       2.51                   92.084548                      42.471550\n",
            "       2.71                   84.851021                      35.616623\n",
            "       1.84                   99.211470                      50.833424\n",
            "       0.67                   50.226553                      62.618160\n",
            "       4.83                  -60.668235                     -79.399658\n",
            "       1.41                   89.454464                      55.164581\n",
            "      15.34                  -49.545867                     -83.565712\n",
            "       9.32                   86.074203                      86.074829\n",
            "       5.28                  -84.432793                     -96.856430\n",
            "       4.64                  -48.175367                     -69.007256\n",
            "       7.15                  -61.909395                     -47.044762\n",
            "      10.01                   99.996916                      92.076660\n",
            "       3.06                   67.301251                      17.890242\n",
            "       7.65                  -27.144045                      -9.956096\n",
            "      11.37                   47.485639                      41.960323\n",
            "      12.55                  -41.865974                      -6.492251\n",
            "       5.91                  -99.750280                     -99.247185\n",
            "      11.28                   53.582679                      45.655865\n",
            "       2.97                   72.356978                      22.448460\n",
            "      13.80                  -98.768834                     -49.477306\n",
            "       9.38                   88.376563                      86.794685\n",
            "      12.42                  -32.391742                      -1.154266\n",
            "       7.30                  -52.249856                     -35.918152\n",
            "      11.46                   41.151436                      38.264778\n",
            "      12.08                   -6.279052                      12.806682\n",
            "       8.88                   63.742399                      73.479446\n",
            "      13.70                  -97.236992                     -47.261456\n",
            "       5.60                  -95.105652                     -98.144440\n",
            "       9.25                   83.146961                      85.235001\n",
            "       3.82                   14.090123                     -23.193930\n",
            "      11.21                   58.141318                      48.530182\n",
            "       2.39                   95.345417                      44.947582\n",
            "      12.99                  -70.153143                     -24.559290\n",
            "       4.51                  -38.992769                     -61.804070\n",
            "       3.16                   61.290705                      12.825578\n",
            "      11.64                   27.899111                      30.873705\n",
            "       3.31                   51.578590                       5.228581\n",
            "       9.78                   98.510933                      91.593719\n",
            "       5.54                  -93.544403                     -97.931366\n",
            "       5.90                  -99.691733                     -99.211617\n",
            "       3.03                   69.025124                      19.409666\n",
            "      13.11                  -76.548321                     -29.486670\n",
            "       1.70                   97.236992                      52.243553\n",
            "      10.89                   76.548321                      61.678371\n",
            "       2.61                   88.741345                      39.401928\n",
            "       5.75                  -98.078528                     -98.677872\n",
            "       0.44                   33.873792                      64.934845\n",
            "       4.68                  -50.904142                     -71.219231\n",
            "       2.59                   89.454464                      40.015831\n",
            "       5.44                  -90.482705                     -97.576271\n",
            "       9.41                   89.454464                      87.154617\n",
            "       3.53                   36.081083                      -6.934114\n",
            "       3.98                    1.570732                     -32.148094\n",
            "       8.42                   32.391742                      46.944031\n",
            "       7.52                  -36.812455                     -19.599159\n",
            "       5.29                  -84.851021                     -96.947227\n",
            "      10.37                   95.807290                      82.891579\n",
            "       1.15                   78.531693                      57.783390\n",
            "       7.36                  -48.175367                     -31.467516\n",
            "       8.36                   27.899111                      42.598511\n",
            "       8.67                   50.226553                      64.609863\n",
            "       1.75                   98.078528                      51.739956\n",
            "       3.42                   43.993917                      -0.766593\n",
            "      13.18                  -79.968466                     -32.360981\n",
            "       4.13                  -10.192446                     -40.541286\n",
            "      12.12                   -9.410831                      11.164210\n",
            "       0.78                   57.500525                      61.510227\n",
            "      11.45                   41.865974                      38.675411\n",
            "      14.75                  -83.146961                     -70.508385\n",
            "      10.54                   91.140328                      76.052620\n",
            "      13.91                  -99.750280                     -51.914745\n",
            "       6.15                  -99.306846                     -98.095642\n",
            "       5.85                  -99.306846                     -99.033691\n",
            "       1.98                   99.987663                      49.423286\n",
            "       0.15                   11.753740                      67.858276\n",
            "       7.86                  -10.973431                       5.621151\n",
            "      11.33                   50.226553                      43.602791\n",
            "       2.74                   83.580736                      34.097240\n",
            "      12.25                  -19.509032                       5.826216\n",
            "      10.31                   97.050647                      85.200645\n",
            "       7.20                  -58.778525                     -43.335926\n",
            "       8.93                   66.718277                      75.425598\n",
            "      13.23                  -82.264052                     -34.396072\n",
            "       6.72                  -84.432793                     -78.698967\n",
            "      12.46                  -35.347484                      -2.796705\n",
            "       4.93                  -66.718277                     -84.745163\n",
            "       6.54                  -91.140328                     -85.717438\n",
            "       1.99                   99.996916                      49.322571\n",
            "       6.07                  -99.848910                     -99.087280\n",
            "      13.30                  -85.264016                     -36.591240\n",
            "       9.06                   73.963109                      80.471130\n",
            "      12.27                  -21.047176                       5.004967\n",
            "      15.92                   -6.279052                     -96.401688\n",
            "       3.51                   37.541557                      -5.812764\n",
            "      11.85                   11.753740                      22.250790\n",
            "       3.81                   14.867243                     -22.633266\n",
            "       8.19                   14.867243                      30.099632\n",
            "       9.83                   99.109975                      92.025146\n",
            "      11.51                   37.541557                      36.211708\n",
            "      10.04                   99.950656                      91.936562\n",
            "\n",
            "AutoKeras Performance Metrics:\n",
            "Mean Absolute Error: 76.63720568393248\n",
            "Root Mean Squared Error: 94.03211848747682\n",
            "Coefficient of Determination (R^2): 0.793784246738753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchdiffeq\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from autokeras import StructuredDataRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the input parameters\n",
        "E = 29000000.0 # Modulus of elasticity (kN/m2)\n",
        "I = 0.00423 # Moment of inertia of the cross-section (m4)\n",
        "Area = 0.69 # Area of the cross-section (m2)\n",
        "L = 4.0 # Length of beam (m)\n",
        "p = 27395.515 # Mass/volume unit (kg/m3)\n",
        "kf = 1500.0 # Modulus of the subgrafe reaction (kN/m2)\n",
        "Pe = 500.0 # External moving load (kN)\n",
        "c = 1.00 # Velocity of the external load (m/sec)\n",
        "\n",
        "# A. STEP: Solution of eigenvalue problem -> Calculation of eigenfrequencies and\n",
        "# eigenvectors: EI*(d4X/dx4)+(kf-wn2*p*Area)*X=0\n",
        "# First eigenfrequency\n",
        "n = 1\n",
        "wn = np.sqrt((1/(p*Area))*((((n**4)*(np.pi**4)*E*I)/(L**4))+kf))\n",
        "# First eigenvector - value in the middle of the beam : x=L/2\n",
        "Pn = ((wn**2*p*Area-kf)/(E*I))**(1/4)\n",
        "X1L2 = np.sin(Pn*(L/2))\n",
        "\n",
        "# B. STEP: Calculation of the generalized parameters qi(t)\n",
        "# (dqi2/dt2)+((wn2*Mn+kf*Nn)/Mn)*qi=(Pe/Mn)*sin(Pn*c*t)\n",
        "wn2 = wn**2\n",
        "Nn = (L/2)-(np.sin(2*Pn*L)/(4*Pn))\n",
        "Mn = p*Area*Nn\n",
        "k = Pe/Mn\n",
        "beta = (wn2*Mn+kf*Nn)/Mn\n",
        "q0 = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "def damped_vibration(t, y):\n",
        "    q1, q2 = y[0], y[1]\n",
        "    dq1dt = q2\n",
        "    dq2dt = k*torch.sin(Pn*c*t) - beta*q1\n",
        "    return torch.stack([dq1dt, dq2dt])\n",
        "\n",
        "# Define the initial state of the system as a PyTorch tensor with the requires_grad flag\n",
        "q0 = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "# Define the time points at which to evaluate the solution\n",
        "t = torch.linspace(0.0, 16.0, 1601)\n",
        "\n",
        "# Solve the differential equation using automatic differentiation\n",
        "q = torchdiffeq.odeint(damped_vibration, q0, t)\n",
        "\n",
        "# Extract the first component of the solution as the predicted output\n",
        "q1 = q[:, 0]\n",
        "\n",
        "A1mat = np.linspace(0, 16.0, 1601)\n",
        "A2mat = X1L2*q1.detach().numpy()*100\n",
        "X_train, X_test, y_train, y_test = train_test_split(A1mat.reshape(-1, 1), A2mat.reshape(-1), test_size=0.15, random_state=42)\n",
        "\n",
        "# Define the AutoKeras model\n",
        "reg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\n",
        "\n",
        "# Search for the best model architecture\n",
        "reg.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# Evaluate the AutoKeras model on the test data\n",
        "y_pred = reg.predict(X_test)\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "rmse = np.sqrt(np.mean(np.square(y_test - y_pred)))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2 Score:\", r2)\n",
        "\n",
        "# Generate predictions using the AutoKeras model\n",
        "y_pred_train = reg.predict(X_train)\n",
        "y_pred_test = reg.predict(X_test)\n",
        "\n",
        "# Plot the predicted values against the true values for the training data\n",
        "plt.scatter(y_train, y_pred_train)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predictions\")\n",
        "plt.title(\"Training Data\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the predicted values against the true values for the test data\n",
        "plt.scatter(y_test, y_pred_test)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predictions\")\n",
        "plt.title(\"Test Data\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fM-5x2fD2Zjr",
        "outputId": "887ee62c-5056-4866-b11f-ae6b62b95e63"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 05s]\n",
            "val_loss: 0.1401214450597763\n",
            "\n",
            "Best val_loss So Far: 0.003429385367780924\n",
            "Total elapsed time: 00h 00m 53s\n",
            "Epoch 1/10\n",
            "43/43 [==============================] - 2s 5ms/step - loss: 0.2993 - mean_squared_error: 0.2993\n",
            "Epoch 2/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0247 - mean_squared_error: 0.0247\n",
            "Epoch 3/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_squared_error: 0.0136\n",
            "Epoch 4/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0131 - mean_squared_error: 0.0131\n",
            "Epoch 5/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0074 - mean_squared_error: 0.0074\n",
            "Epoch 6/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0170 - mean_squared_error: 0.0170\n",
            "Epoch 7/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0098 - mean_squared_error: 0.0098\n",
            "Epoch 8/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0063 - mean_squared_error: 0.0063\n",
            "Epoch 9/10\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
            "Epoch 10/10\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.0164 - mean_squared_error: 0.0164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 3ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "MAE: 0.59720784\n",
            "RMSE: 0.7342295\n",
            "R2 Score: 0.9529030795735665\n",
            "43/43 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}